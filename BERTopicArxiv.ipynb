{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c3041a",
   "metadata": {},
   "source": [
    "# BERTopic Arxiv\n",
    "- https://www.maartengrootendorst.com/blog/bertopic/\n",
    "- https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html\n",
    "- https://www.kaggle.com/code/maartengr/topic-modeling-arxiv-abstract-with-bertopic\n",
    "- plus SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d5628",
   "metadata": {},
   "source": [
    "%%capture - cell magic, %%capture , which captures the stdout/stderr of a cell. With this magic you can discard these streams or store them in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04057534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge hdbscan -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109e5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install python-dev-tools --user --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5391885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b227e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9d315",
   "metadata": {},
   "source": [
    "## SBERT\n",
    "Sentense transformation and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321ba1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe408c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e73bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d6343f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0e4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63209ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce955c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a872727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a1c4b",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403500e6",
   "metadata": {},
   "source": [
    "We need to split the text into iterable segments (sentences), which will be vectorised (embedded). SBERT will search for similarity between the sentences to find the closest that will formulate the topics in BERTopic.<br>\n",
    "We are choosing from pretrained multilingual models for unsupervised learning, but can train/tune our own, based on our samples (see the examples in SBERT MMulti and SBERT official site sbert.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92d27e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test corpus\n",
    "corp = '''Jeg har sammen med mine to medstuderende Sofie Amalie Landt og Benjamin Aizen Kongshaug været i praktik hos konsulentfirmaet HBS Economics. Virksomheden blev stiftet i 2015, som Høj-bjerre Brauer Schultz ApS, og skiftede i 2020 navn til HBS Economics. I dag er virksomheden udeluk-kende ledet af de to partnere Andreas Højbjerre og Esben Schultz, som hver har ansvaret for virk-somhedens to afdelinger, data science og økonomi. Virksomheden har kontor i København og består af 5 fastansatte og et større antal studentermedhjælper og praktikanter fordelt mellem de to afde-linger.  HBS har en vision om at tilbyde den bedste økonomiske rådgivning og analyse i Norden, med det mål at kunne skabe et bedre samfund. Det vil de opnå ved at tilbyde analyse og rådgivning af høj kvalitet, så deres kunder kan træffe beslutninger, skabe forandringer og fastsætte nye agendaer. Før i tiden henvendte HBS sig primært til offentlige autoriteter og organisationer, men i 2020, efter et større eksternt konsulentstop i det offentlige, omlagde HBS deres strategi som betyder at de i dag primært tilbyder deres ydelser til private virksomheder, herunder Lundbeck, DFDS og Glad Fon-den.  HBS har udviklet et værktøj til at håndtere og udtrække struktureret information fra store mængder af ustrukturerede data for at skabe ny viden og værdi for deres kunder. Det er disse opgaver, som Data Science afdelingen varetager. Ved at anvende metoder som data crawling, statistisk tekstana-lyse og data mining på f.eks. alle online job opslag i Norden, kan de tilbyde et bredt udvalg af unikke data drevne analyser. I samarbejde med IDA har HBS valgt at investere og udvikle et nyt datadrevet projekt kaldet JobSpire. JobSpire er en jobportal, der skal gøre det nemmere for jobsøgende at in-spirere, og lade sig inspirere, af andre med en lignende karriere. JobSpire skal på sigt blive en separat virksomhed. Vi har i vores praktikforløb været en del af Data Science, og har arbejdet på et potenti-elt anbefalingssystem, som skal integreres i JobSpire.  2. Arbejdsopgaver og refleksion af disse i forhold til teorier og modeller  JobSpire er et nyopstartet projekt og er derfor stadig i udviklingsfasen, hvilket har betydet at vi fra start selv måtte undersøge mulighederne for at lave et anbefalingssystem, sætte et udviklingsmiljø op, samt finde midlertidig test data.   Amanda Juhl Hansen  \n",
    " 4/8 Hele applikationen er bygget op i Python og til det har jeg kunne bruge min viden herom fra Data Science. Applikationen er desuden tilknyttet en Postgres og en Neo4j database. Med viden fra 2. semester omkring Docker og containerization havde jeg sammen med mine to medstuderende op-rettet en Docker container til hver af de to databaser. Postgres databasen er en er en midlertidig erstatning for den i JobSpire og Neo4j databasen skulle danne grundlag for vores anbefalingssystem. Til opsætning af disse databaser, brugte jeg min viden og forståelse om databaser, som jeg havde tilegnet mig på 1. semester.  En af mine første arbejdsopgaver lød på at undersøge den test data vi fandt. Det gjorde jeg for at få et indblik og en forståelse for dataens struktur. Forberedelsen inkluderede at udtrække data fra kolonner til nye dataframes, så vi kunne arbejde med dataen. Herefter skulle de hver især renses og transformeres, for at berige dataen mest muligt. Det var en længere og omdiskuteret proces. For at kunne forbedrede dataen har jeg brugt min viden fra studiet om data preparation og operationer på dataframes.   Fra studiet har jeg fået kendskab til Cypher Language og her lærte jeg at lave simple cypher queries. Det har været brugbart i forbindelse med vores Neo4j database, hvor en af mine andre opgaver var at lave mere komplekse queries til at udtrække forskellige anbefalinger og inspiration til karriereveje og jobskifte. Derudover har jeg også tilføjet og brugt plugins som APOC og GDS, som jeg også har anvendt og fået kendskab til gennem studiet. En af mine sidste arbejdsopgaver har været at skulle opsætte API’er som skal udstille endpoints fra applikationen til JobSpire. Kommunikation mellem fleres services gennem et API er viden jeg har tillært mig på 2. semester, og jeg har derfor kunnet bruge denne viden til arbejdsopgaven. Derudover har jeg undervejs i udviklingsprocessen udført linting af vores applikation, som jeg har fået viden og kendskab til gennem faget test, hvor jeg lærte om static testing.  3. Læringsmål  3.1 Viden   Den første uge blev jeg introduceret til begge afdelinger i virksomheden. Den afdeling jeg skulle være i, består af 3 fastansatte som hver har deres rolle og ansvarsområde i teamet. Desuden blev vi Amanda Juhl Hansen  \n",
    " 5/8 introduceret til det projekt som jeg sammen med mine medpraktikanter skulle arbejde på under praktikken. Den første mandag i hver måned afholdes et møde for begge afdelinger, hvor alle for-klarer hvilke opgaver der arbejdes på og eventuelle udfordringer der måtte være i samme forbin-delse. Formålet med mødet var at sikre at alle var indforstået med hvilke igangværende opgaver, sørge for at deadlines og aftaler blev overholdt, samt planlægge fremadrettet. Dagligdagen i mit team bar også præg af sparing mellem udviklere i form af flere faglige diskussioner, i forbindelse med problemløsninger.  3.2 Færdigheder  Under praktikken har jeg og mine medpraktikanter arbejdet i sprints af to uger, for at følge den samme proces som resten af Data Science afdelingen. Derudover anvender de KanBan Board meto-den og vi valgte at gøre det samme. Vi havde et fast møde, efter et sprints afslutning, hvor vi defi-nerede og tilføjede den rette mængde Cards til et kommende sprint. Det betød at vi kunne struktu-rere vores sprint og ved at uddelegere opgaver, kunne vi også strukturere vores daglige arbejde. Jeg brugte derfor vores KanBan board effektivt og dagligt til at planlægge og få et overblik over hvor langt jeg var nået og hvad der manglede for at færdiggøre opgaven. Mine medpraktikanter og jeg var gode til at spare med hinanden og give feedback, som resulterede i en masse små rettelser henvendt mod forbedring eller refaktorering. Afhængig af hvor omfattende rettelserne var, blev den enten rettet med det samme, eller der blev defineret et nyt card med opgaven, så fokus forblev på den igangværende.  Jeg blev undervejs i praktikken konfronteret med flere problemstillinger, som jeg sammen med mine medpraktikanter skulle finde en løsning på. En af de problematikker var at vi havde et relativt lille datasæt og at en karrieresti blev repræsenteret af flere rækker. Det betød at jeg i de tilfælde hvor en række indeholdt NaN værdier, ikke blot kunne slette rækken. Hvis jeg slettede en række (et kar-rieretrin i en karrieresti), ville det give et misledende billede og hvis jeg slettede karrierestier, vil vores datasæt blive endnu mindre. Løsningen indebar derfor at lave en metode, der udfyldte tomme startdatoer med dags dato for 2 år siden og tomme slutdatoer blev udfyldt med dagsdato, hvis det var det nuværende job, for at kunne udregne, hvor lang tid man havde haft et job og samle det i et Amanda Juhl Hansen  \n",
    " 6/8 interval. Det gav langt større værdi at erstatte disse NaN-værdier med en smule misvisende data, fremfor at slette flere karrierestier.1   Efter at have lagt alle joberfaringer i Graph database opstod en anden problematik. Personer med to erhvervserfaringer efterfulgt af hinanden, med samme jobtitel og jobområde, resulterede i at noden i databasen havde en relation til sig selv. Det skyldes at det er jobtitel og jobområde der differentierer joberfaringer. Løsningen var at jeg lavede en metode, der lagde varigheden af hvor længe man havde haft jobbet sammen fra den nuværende og forgående, og derefter slettede den foregående joberfaring, for at undgå at der blev skabt en relation til den samme joberfaring. Vi øn-skede kun at gøre dette, hvis det var joberfaringer der lå lige efter hinanden2.   Vi oplevede at vores danske oversættelse af jobtitler ikke var nok, da det viste sig at flere jobtitler i vores testdata ikke kun var på dansk eller engelsk. Problematikken var at hvis der ikke fandtes en dansk oversættelse i databasen, vil både FuzzyMatch og Semantic Search ikke kunne give et kvalifi-ceret bud. Derfor testede vi om en engelsk oversættelse af jobtitlerne ville give et bedre resultat, hvilket var tilfældet og vi valgte derfor at bruge denne løsning og implementere det i den nuværende metode.  3.3 Kompetencer  Da jeg på studiet primært har programmeret objektorienteret og haft data science som valgfag, havde jeg en smule kendskab til Python. I samarbejde med mine medstuderende i praktikken har jeg haft rig mulighed for at udvide disse kompetencer, eftersom Python også er trivielt til funktionel programmering. Samtidig har jeg kunnet bruge mine kompetencer om objekt orienteret program-mering og Object Relational Mapping til at udvikle og strukturere Python objekter med dets relati-oner. For at mappe disse Python objekter og relationer ind i Neo4j-databasen, har jeg brugt tid på at læse og forstå dokumentationen om Neomodel, som er et Python bibliotek der tillader Object Graph Mapping. I forbindelse med vores anbefalingssystem, har jeg lavet en række komplekse cy-pher queries, der skal bruges til at give job inspiration til brugerne. Jeg har bl.a. lavet en cypher  1 Se logbog d. 9.2.2022 2 Se logbog: d. 4.3.2022 – 7.3.2022 Amanda Juhl Hansen  \n",
    " 7/8 query der ved brug af GDS finder de korteste ruter, fra ens nuværende jobposition til en fremtidig. Ruterne er sorteret efter popularitet. Derudover har jeg lavet en cypher query der udtrækker kendte menneskers karrierestier, indenfor en bestemt distance med udgangspunkt i ens nuværende jobpo-sition. De sorteres efter hvor mange jobskifte der er mellem nuværende job, og slutningen af den kendt persons karrieresti.  Igennem en længere proces af at rense og forbedrede data under praktikken, har jeg brugt forskel-lige NumPy-operationer, samt filter og map funktioner. Ved at implementere disse flere steder har jeg opnået viden og kompetencer indenfor iterative operationer på DataFrames, som jeg før kun havde i begrænset omfang fra studiet.  4. Refleksion over praktikforløbets udbytte for virksomheden samt for mig selv  Virksomheden har ikke før arbejdet med Graph databaser og derfor har de begrænset erfaring med disse. Derfor har vores praktikophold skabt stor værdi for virksomheden, da vi har kunnet undersøge hvilke muligheder der er for at implementere en Graph database, som anbefalingssystem til Job-Spire. Tilsvarende har jeg, som praktikant, fået en dybere indsigt i Graph databaser, både hvordan de kan anvendes til datahåndtering og som potentielt anbefalingssystem. Vi har formået at opfylde virksomheden ønsker til projektet, samt udvikle en fuldt fungerende applikation der på sigt kan blive implementeret i JobSpire1.  Set i bakspejlet var jeg og mine medstuderende ikke særligt integreret i data science afdelingens hverdag og møder. Mit indtryk er, at det primært skyldes at afdelingen er relativ lille med få udvik-lere, som varetager andre opgaver, og det faktum at vi arbejder på et selvstændigt og nyt projekt. Det har betydet at jeg og mine medpraktikanter har haft ansvaret for projektet og derfor også har skulle træffe nogle store beslutninger på egen hånd. Det har lært mig at stå til ansvar for egne be-slutninger og have begrundelse for de valg der blev truffet under udviklingen, samt hvad et godt og kommunikativt samarbejde er, da jeg har haft en tæt dialog og samarbejde med mine medstude-rende under praktikken. Det kunne have givet mere værdi og et større udbytte for virksomheden,  1 Se logbog: d. 9.3.2022 Amanda Juhl Hansen  \n",
    " 8/8 hvis de havde valgt at være mere inde i processen, samt være med til at træffe beslutninger for at justere retningen af projektet.   Mit praktikforløb hos HBS har været med til at give mig en større indsigt i hvilket område af min profession, jeg ønsker at arbejde med i fremtiden. Det skyldes primært det store udbytte jeg har fået igennem mit arbejde med Python, NLP, GNN og Graph databaser som anbefalingssystem. Det har udviklet mine faglige kompetencer og dermed skabt en stigende interesse for dette.         \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3311a6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jeg har sammen med mine to medstuderende Sofie Amalie Landt og Benjamin Aizen Kongshaug været i praktik hos konsulentfirmaet HBS Economics.',\n",
       " 'Virksomheden blev stiftet i 2015, som Høj-bjerre Brauer Schultz ApS, og skiftede i 2020 navn til HBS Economics.',\n",
       " 'I dag er virksomheden udeluk-kende ledet af de to partnere Andreas Højbjerre og Esben Schultz, som hver har ansvaret for virk-somhedens to afdelinger, data science og økonomi.',\n",
       " 'Virksomheden har kontor i København og består af 5 fastansatte og et større antal studentermedhjælper og praktikanter fordelt mellem de to afde-linger.',\n",
       " 'HBS har en vision om at tilbyde den bedste økonomiske rådgivning og analyse i Norden, med det mål at kunne skabe et bedre samfund.',\n",
       " 'Det vil de opnå ved at tilbyde analyse og rådgivning af høj kvalitet, så deres kunder kan træffe beslutninger, skabe forandringer og fastsætte nye agendaer.',\n",
       " 'Før i tiden henvendte HBS sig primært til offentlige autoriteter og organisationer, men i 2020, efter et større eksternt konsulentstop i det offentlige, omlagde HBS deres strategi som betyder at de i dag primært tilbyder deres ydelser til private virksomheder, herunder Lundbeck, DFDS og Glad Fon-den.',\n",
       " 'HBS har udviklet et værktøj til at håndtere og udtrække struktureret information fra store mængder af ustrukturerede data for at skabe ny viden og værdi for deres kunder.',\n",
       " 'Det er disse opgaver, som Data Science afdelingen varetager.',\n",
       " 'Ved at anvende metoder som data crawling, statistisk tekstana-lyse og data mining på f.eks.',\n",
       " 'alle online job opslag i Norden, kan de tilbyde et bredt udvalg af unikke data drevne analyser.',\n",
       " 'I samarbejde med IDA har HBS valgt at investere og udvikle et nyt datadrevet projekt kaldet JobSpire.',\n",
       " 'JobSpire er en jobportal, der skal gøre det nemmere for jobsøgende at in-spirere, og lade sig inspirere, af andre med en lignende karriere.',\n",
       " 'JobSpire skal på sigt blive en separat virksomhed.',\n",
       " 'Vi har i vores praktikforløb været en del af Data Science, og har arbejdet på et potenti-elt anbefalingssystem, som skal integreres i JobSpire.',\n",
       " '2.',\n",
       " 'Arbejdsopgaver og refleksion af disse i forhold til teorier og modeller  JobSpire er et nyopstartet projekt og er derfor stadig i udviklingsfasen, hvilket har betydet at vi fra start selv måtte undersøge mulighederne for at lave et anbefalingssystem, sætte et udviklingsmiljø op, samt finde midlertidig test data.',\n",
       " 'Amanda Juhl Hansen  \\n 4/8 Hele applikationen er bygget op i Python og til det har jeg kunne bruge min viden herom fra Data Science.',\n",
       " 'Applikationen er desuden tilknyttet en Postgres og en Neo4j database.',\n",
       " 'Med viden fra 2. semester omkring Docker og containerization havde jeg sammen med mine to medstuderende op-rettet en Docker container til hver af de to databaser.',\n",
       " 'Postgres databasen er en er en midlertidig erstatning for den i JobSpire og Neo4j databasen skulle danne grundlag for vores anbefalingssystem.',\n",
       " 'Til opsætning af disse databaser, brugte jeg min viden og forståelse om databaser, som jeg havde tilegnet mig på 1. semester.',\n",
       " 'En af mine første arbejdsopgaver lød på at undersøge den test data vi fandt.',\n",
       " 'Det gjorde jeg for at få et indblik og en forståelse for dataens struktur.',\n",
       " 'Forberedelsen inkluderede at udtrække data fra kolonner til nye dataframes, så vi kunne arbejde med dataen.',\n",
       " 'Herefter skulle de hver især renses og transformeres, for at berige dataen mest muligt.',\n",
       " 'Det var en længere og omdiskuteret proces.',\n",
       " 'For at kunne forbedrede dataen har jeg brugt min viden fra studiet om data preparation og operationer på dataframes.',\n",
       " 'Fra studiet har jeg fået kendskab til Cypher Language og her lærte jeg at lave simple cypher queries.',\n",
       " 'Det har været brugbart i forbindelse med vores Neo4j database, hvor en af mine andre opgaver var at lave mere komplekse queries til at udtrække forskellige anbefalinger og inspiration til karriereveje og jobskifte.',\n",
       " 'Derudover har jeg også tilføjet og brugt plugins som APOC og GDS, som jeg også har anvendt og fået kendskab til gennem studiet.',\n",
       " 'En af mine sidste arbejdsopgaver har været at skulle opsætte API’er som skal udstille endpoints fra applikationen til JobSpire.',\n",
       " 'Kommunikation mellem fleres services gennem et API er viden jeg har tillært mig på 2. semester, og jeg har derfor kunnet bruge denne viden til arbejdsopgaven.',\n",
       " 'Derudover har jeg undervejs i udviklingsprocessen udført linting af vores applikation, som jeg har fået viden og kendskab til gennem faget test, hvor jeg lærte om static testing.',\n",
       " '3.',\n",
       " 'Læringsmål  3.1 Viden   Den første uge blev jeg introduceret til begge afdelinger i virksomheden.',\n",
       " 'Den afdeling jeg skulle være i, består af 3 fastansatte som hver har deres rolle og ansvarsområde i teamet.',\n",
       " 'Desuden blev vi Amanda Juhl Hansen  \\n 5/8 introduceret til det projekt som jeg sammen med mine medpraktikanter skulle arbejde på under praktikken.',\n",
       " 'Den første mandag i hver måned afholdes et møde for begge afdelinger, hvor alle for-klarer hvilke opgaver der arbejdes på og eventuelle udfordringer der måtte være i samme forbin-delse.',\n",
       " 'Formålet med mødet var at sikre at alle var indforstået med hvilke igangværende opgaver, sørge for at deadlines og aftaler blev overholdt, samt planlægge fremadrettet.',\n",
       " 'Dagligdagen i mit team bar også præg af sparing mellem udviklere i form af flere faglige diskussioner, i forbindelse med problemløsninger.',\n",
       " '3.2 Færdigheder  Under praktikken har jeg og mine medpraktikanter arbejdet i sprints af to uger, for at følge den samme proces som resten af Data Science afdelingen.',\n",
       " 'Derudover anvender de KanBan Board meto-den og vi valgte at gøre det samme.',\n",
       " 'Vi havde et fast møde, efter et sprints afslutning, hvor vi defi-nerede og tilføjede den rette mængde Cards til et kommende sprint.',\n",
       " 'Det betød at vi kunne struktu-rere vores sprint og ved at uddelegere opgaver, kunne vi også strukturere vores daglige arbejde.',\n",
       " 'Jeg brugte derfor vores KanBan board effektivt og dagligt til at planlægge og få et overblik over hvor langt jeg var nået og hvad der manglede for at færdiggøre opgaven.',\n",
       " 'Mine medpraktikanter og jeg var gode til at spare med hinanden og give feedback, som resulterede i en masse små rettelser henvendt mod forbedring eller refaktorering.',\n",
       " 'Afhængig af hvor omfattende rettelserne var, blev den enten rettet med det samme, eller der blev defineret et nyt card med opgaven, så fokus forblev på den igangværende.',\n",
       " 'Jeg blev undervejs i praktikken konfronteret med flere problemstillinger, som jeg sammen med mine medpraktikanter skulle finde en løsning på.',\n",
       " 'En af de problematikker var at vi havde et relativt lille datasæt og at en karrieresti blev repræsenteret af flere rækker.',\n",
       " 'Det betød at jeg i de tilfælde hvor en række indeholdt NaN værdier, ikke blot kunne slette rækken.',\n",
       " 'Hvis jeg slettede en række (et kar-rieretrin i en karrieresti), ville det give et misledende billede og hvis jeg slettede karrierestier, vil vores datasæt blive endnu mindre.',\n",
       " 'Løsningen indebar derfor at lave en metode, der udfyldte tomme startdatoer med dags dato for 2 år siden og tomme slutdatoer blev udfyldt med dagsdato, hvis det var det nuværende job, for at kunne udregne, hvor lang tid man havde haft et job og samle det i et Amanda Juhl Hansen  \\n 6/8 interval.',\n",
       " 'Det gav langt større værdi at erstatte disse NaN-værdier med en smule misvisende data, fremfor at slette flere karrierestier.1   Efter at have lagt alle joberfaringer i Graph database opstod en anden problematik.',\n",
       " 'Personer med to erhvervserfaringer efterfulgt af hinanden, med samme jobtitel og jobområde, resulterede i at noden i databasen havde en relation til sig selv.',\n",
       " 'Det skyldes at det er jobtitel og jobområde der differentierer joberfaringer.',\n",
       " 'Løsningen var at jeg lavede en metode, der lagde varigheden af hvor længe man havde haft jobbet sammen fra den nuværende og forgående, og derefter slettede den foregående joberfaring, for at undgå at der blev skabt en relation til den samme joberfaring.',\n",
       " 'Vi øn-skede kun at gøre dette, hvis det var joberfaringer der lå lige efter hinanden2.',\n",
       " 'Vi oplevede at vores danske oversættelse af jobtitler ikke var nok, da det viste sig at flere jobtitler i vores testdata ikke kun var på dansk eller engelsk.',\n",
       " 'Problematikken var at hvis der ikke fandtes en dansk oversættelse i databasen, vil både FuzzyMatch og Semantic Search ikke kunne give et kvalifi-ceret bud.',\n",
       " 'Derfor testede vi om en engelsk oversættelse af jobtitlerne ville give et bedre resultat, hvilket var tilfældet og vi valgte derfor at bruge denne løsning og implementere det i den nuværende metode.',\n",
       " '3.3 Kompetencer  Da jeg på studiet primært har programmeret objektorienteret og haft data science som valgfag, havde jeg en smule kendskab til Python.',\n",
       " 'I samarbejde med mine medstuderende i praktikken har jeg haft rig mulighed for at udvide disse kompetencer, eftersom Python også er trivielt til funktionel programmering.',\n",
       " 'Samtidig har jeg kunnet bruge mine kompetencer om objekt orienteret program-mering og Object Relational Mapping til at udvikle og strukturere Python objekter med dets relati-oner.',\n",
       " 'For at mappe disse Python objekter og relationer ind i Neo4j-databasen, har jeg brugt tid på at læse og forstå dokumentationen om Neomodel, som er et Python bibliotek der tillader Object Graph Mapping.',\n",
       " 'I forbindelse med vores anbefalingssystem, har jeg lavet en række komplekse cy-pher queries, der skal bruges til at give job inspiration til brugerne.',\n",
       " 'Jeg har bl.a.',\n",
       " 'lavet en cypher  1 Se logbog d. 9.2.2022 2 Se logbog: d. 4.3.2022 – 7.3.2022 Amanda Juhl Hansen  \\n 7/8 query der ved brug af GDS finder de korteste ruter, fra ens nuværende jobposition til en fremtidig.',\n",
       " 'Ruterne er sorteret efter popularitet.',\n",
       " 'Derudover har jeg lavet en cypher query der udtrækker kendte menneskers karrierestier, indenfor en bestemt distance med udgangspunkt i ens nuværende jobpo-sition.',\n",
       " 'De sorteres efter hvor mange jobskifte der er mellem nuværende job, og slutningen af den kendt persons karrieresti.',\n",
       " 'Igennem en længere proces af at rense og forbedrede data under praktikken, har jeg brugt forskel-lige NumPy-operationer, samt filter og map funktioner.',\n",
       " 'Ved at implementere disse flere steder har jeg opnået viden og kompetencer indenfor iterative operationer på DataFrames, som jeg før kun havde i begrænset omfang fra studiet.',\n",
       " '4.',\n",
       " 'Refleksion over praktikforløbets udbytte for virksomheden samt for mig selv  Virksomheden har ikke før arbejdet med Graph databaser og derfor har de begrænset erfaring med disse.',\n",
       " 'Derfor har vores praktikophold skabt stor værdi for virksomheden, da vi har kunnet undersøge hvilke muligheder der er for at implementere en Graph database, som anbefalingssystem til Job-Spire.',\n",
       " 'Tilsvarende har jeg, som praktikant, fået en dybere indsigt i Graph databaser, både hvordan de kan anvendes til datahåndtering og som potentielt anbefalingssystem.',\n",
       " 'Vi har formået at opfylde virksomheden ønsker til projektet, samt udvikle en fuldt fungerende applikation der på sigt kan blive implementeret i JobSpire1.',\n",
       " 'Set i bakspejlet var jeg og mine medstuderende ikke særligt integreret i data science afdelingens hverdag og møder.',\n",
       " 'Mit indtryk er, at det primært skyldes at afdelingen er relativ lille med få udvik-lere, som varetager andre opgaver, og det faktum at vi arbejder på et selvstændigt og nyt projekt.',\n",
       " 'Det har betydet at jeg og mine medpraktikanter har haft ansvaret for projektet og derfor også har skulle træffe nogle store beslutninger på egen hånd.',\n",
       " 'Det har lært mig at stå til ansvar for egne be-slutninger og have begrundelse for de valg der blev truffet under udviklingen, samt hvad et godt og kommunikativt samarbejde er, da jeg har haft en tæt dialog og samarbejde med mine medstude-rende under praktikken.',\n",
       " 'Det kunne have givet mere værdi og et større udbytte for virksomheden,  1 Se logbog: d. 9.3.2022 Amanda Juhl Hansen  \\n 8/8 hvis de havde valgt at være mere inde i processen, samt være med til at træffe beslutninger for at justere retningen af projektet.',\n",
       " 'Mit praktikforløb hos HBS har været med til at give mig en større indsigt i hvilket område af min profession, jeg ønsker at arbejde med i fremtiden.',\n",
       " 'Det skyldes primært det store udbytte jeg har fået igennem mit arbejde med Python, NLP, GNN og Graph databaser som anbefalingssystem.',\n",
       " 'Det har udviklet mine faglige kompetencer og dermed skabt en stigende interesse for dette.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strip in sentences\n",
    "sentences = nltk.sent_tokenize(corp)\n",
    "\n",
    "# strip leading and trailing spaces\n",
    "sentencies = [sentence.strip() for sentence in sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7955c9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b0a5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jeg har sammen med mine to medstuderende Sofie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virksomheden blev stiftet i 2015, som Høj-bjer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dag er virksomheden udeluk-kende ledet af de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virksomheden har kontor i København og består ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HBS har en vision om at tilbyde den bedste øko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Det har lært mig at stå til ansvar for egne be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Det kunne have givet mere værdi og et større u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Mit praktikforløb hos HBS har været med til at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Det skyldes primært det store udbytte jeg har ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Det har udviklet mine faglige kompetencer og d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence\n",
       "0   Jeg har sammen med mine to medstuderende Sofie...\n",
       "1   Virksomheden blev stiftet i 2015, som Høj-bjer...\n",
       "2   I dag er virksomheden udeluk-kende ledet af de...\n",
       "3   Virksomheden har kontor i København og består ...\n",
       "4   HBS har en vision om at tilbyde den bedste øko...\n",
       "..                                                ...\n",
       "81  Det har lært mig at stå til ansvar for egne be...\n",
       "82  Det kunne have givet mere værdi og et større u...\n",
       "83  Mit praktikforløb hos HBS har været med til at...\n",
       "84  Det skyldes primært det store udbytte jeg har ...\n",
       "85  Det har udviklet mine faglige kompetencer og d...\n",
       "\n",
       "[86 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(sentences)\n",
    "data.columns=['sentence']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0664e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative get test text\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# text = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94b82c",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96899f31",
   "metadata": {},
   "source": [
    "most important hyperparameters in BERTopic:\n",
    "- language - \"english\" (model is \"distilbert-base-nli-stsb-mean-tokens\") or \"multilingual\" (\"xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "- top_n_words - number of words per topic to extract (opt 10-20)\n",
    "- n_gram_range - the number of words in topic representation (New York is n_gram_range=(2, 2))\n",
    "- min_topic_size - the minimum size of a topic (less - more topics; more fewer topics, default 10)\n",
    "- nr_topics - the number of topics to reduced to, or auto\n",
    "- low_memory -  to True for less memory used in computation, no probs\n",
    "- calculate_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b0e1b",
   "metadata": {},
   "source": [
    "Make sure that the embeddings are a numpy array with shape: (len(docs), vector_dim) where vector_dim is the dimensionality of the vector embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70de2928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cfc186be0d4507a99b6fde15de7a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0fe39/.gitattributes:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da0bb11313c4feaad34c2524dbf6d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4ed2cbc7724090b9a697cadc0ed866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)83e900fe39/README.md:   0%|          | 0.00/3.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65817e812ab24e8e9b5543c50587e57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e900fe39/config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de47c1bd116f41d397271d61a4bb0091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a52c4867dd64ee6b028184dbdbf9fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bae28545dad45bfa1510e3b23a12010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6f8632ae1f4c888d4a8c12bfa0197d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ncepiece.bpe.model\";:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948ca30658434d25b62ec818818529b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93951164e15a4da89e296c0db4b81f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6567ad1309d94f4e810fc2d4219f607c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7c7fe4da12448685bfac816acbc601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"unigram.json\";:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890b2dbd627c4e1b9c7a92b35ffb77b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)900fe39/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose model, https://www.sbert.net/docs/pretrained_models.html \n",
    "# model = SentenceTransformer('distiluse-base-multilingual-cased-v2') # better than 2 in Danish, not in English (see tests in SBERT Multi)\n",
    "# model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') # comparable to 3\n",
    "sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=\"cpu\")\n",
    "# sentence_model = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aa82279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own embeddings to apply later as a model\n",
    "# embeddings = sentence_model.encode(sentences, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9226f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our topic model using our pre-trained sentence-transformers embeddings from above\n",
    "# topics, probs = topic_model.fit_transform(sentences, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6edcd43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create topic model\n",
    "topic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, \n",
    "                       verbose=True, embedding_model=sentence_model,  min_topic_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2648c10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfc432b91e14bd7a0591b65acfd1d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 11:03:10,475 - BERTopic - Transformed documents to Embeddings\n",
      "2023-03-29 11:03:16,062 - BERTopic - Reduced dimensionality\n",
      "2023-03-29 11:03:16,093 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# fit the model to the corpus\n",
    "topics, probs = topic_model.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e14bd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa380c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence):\n",
    "    embedding = sentence_model.encode([sentence])\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24c61bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jeg har sammen med mine to medstuderende Sofie...</td>\n",
       "      <td>[0.10439316, 0.22922207, -0.22167933, -0.05284...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virksomheden blev stiftet i 2015, som Høj-bjer...</td>\n",
       "      <td>[-0.26674688, 0.034413755, -0.18169071, 0.1152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dag er virksomheden udeluk-kende ledet af de...</td>\n",
       "      <td>[-0.0933343, 0.124671414, -0.18407618, -0.0527...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virksomheden har kontor i København og består ...</td>\n",
       "      <td>[-0.09658054, 0.3123514, -0.13841988, -0.07573...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HBS har en vision om at tilbyde den bedste øko...</td>\n",
       "      <td>[-0.24162601, 0.21646976, -0.31479874, -0.1944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Det har lært mig at stå til ansvar for egne be...</td>\n",
       "      <td>[0.110346906, 0.087466456, -0.11196691, -0.023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Det kunne have givet mere værdi og et større u...</td>\n",
       "      <td>[-0.038013376, 0.15966125, -0.25663814, -0.083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Mit praktikforløb hos HBS har været med til at...</td>\n",
       "      <td>[0.01944973, 0.23123045, -0.27309445, -0.08802...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Det skyldes primært det store udbytte jeg har ...</td>\n",
       "      <td>[-0.3442768, -0.15640004, -0.13993436, -0.1019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Det har udviklet mine faglige kompetencer og d...</td>\n",
       "      <td>[0.15529926, 0.1748171, -0.28702128, -0.064813...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  \\\n",
       "0   Jeg har sammen med mine to medstuderende Sofie...   \n",
       "1   Virksomheden blev stiftet i 2015, som Høj-bjer...   \n",
       "2   I dag er virksomheden udeluk-kende ledet af de...   \n",
       "3   Virksomheden har kontor i København og består ...   \n",
       "4   HBS har en vision om at tilbyde den bedste øko...   \n",
       "..                                                ...   \n",
       "81  Det har lært mig at stå til ansvar for egne be...   \n",
       "82  Det kunne have givet mere værdi og et større u...   \n",
       "83  Mit praktikforløb hos HBS har været med til at...   \n",
       "84  Det skyldes primært det store udbytte jeg har ...   \n",
       "85  Det har udviklet mine faglige kompetencer og d...   \n",
       "\n",
       "                                           embeddings  \n",
       "0   [0.10439316, 0.22922207, -0.22167933, -0.05284...  \n",
       "1   [-0.26674688, 0.034413755, -0.18169071, 0.1152...  \n",
       "2   [-0.0933343, 0.124671414, -0.18407618, -0.0527...  \n",
       "3   [-0.09658054, 0.3123514, -0.13841988, -0.07573...  \n",
       "4   [-0.24162601, 0.21646976, -0.31479874, -0.1944...  \n",
       "..                                                ...  \n",
       "81  [0.110346906, 0.087466456, -0.11196691, -0.023...  \n",
       "82  [-0.038013376, 0.15966125, -0.25663814, -0.083...  \n",
       "83  [0.01944973, 0.23123045, -0.27309445, -0.08802...  \n",
       "84  [-0.3442768, -0.15640004, -0.13993436, -0.1019...  \n",
       "85  [0.15529926, 0.1748171, -0.28702128, -0.064813...  \n",
       "\n",
       "[86 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['embeddings'] = data['sentence'].apply(get_sentence_embeddings)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "323ad816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['embeddings'][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8934b9",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0a54c",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea8afb",
   "metadata": {},
   "source": [
    "Uniform Manifold Approximation and Projection (__UMAP__) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data (see https://umap-learn.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0faedf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9212449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6aee777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b6b571f30540249fce4eb40d8291f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 11:03:19,034 - BERTopic - Transformed documents to Embeddings\n",
      "2023-03-29 11:03:20,577 - BERTopic - Reduced dimensionality\n",
      "2023-03-29 11:03:20,586 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# Create topic model\n",
    "topic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, \n",
    "                       verbose=True, embedding_model=sentence_model,  min_topic_size=3,\n",
    "                       umap_model=umap_model).fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfe2414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "104035c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f4/g09hx1n5747dttb_p424s85w0000gn/T/ipykernel_7003/2531228096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m umap_embeddings = umap.UMAP(n_neighbors=15, \n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             metric='cosine').fit_transform(embeddings)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# better alternative\n",
    "import umap\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc439e68",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84463df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = PCA(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97147aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topic model\n",
    "topic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, \n",
    "                       verbose=True, embedding_model=sentence_model,  min_topic_size=3,\n",
    "                       umap_model=dim_model).fit(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c42847",
   "metadata": {},
   "source": [
    "### Model Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47b73e",
   "metadata": {},
   "source": [
    "There are a number of attributes that you can access after having trained your BERTopic model:\n",
    "\n",
    "\n",
    "Attribute\tDescription\n",
    "- topics_\tThe topics that are generated for each document after training or updating the topic model.\n",
    "- probabilities_\tThe probabilities that are generated for each document if HDBSCAN is used.\n",
    "- topic_sizes_\tThe size of each topic\n",
    "- topic_mapper_\tA class for tracking topics and their mappings anytime they are merged/reduced.\n",
    "- topic_representations_\tThe top n terms per topic and their respective c-TF-IDF values.\n",
    "- c_tf_idf_\tThe topic-term matrix as calculated through c-TF-IDF.\n",
    "- topic_labels_\tThe default labels for each topic.\n",
    "- custom_labels_\tCustom labels for each topic as generated through .set_topic_labels.\n",
    "- topic_embeddings_\tThe embeddings for each topic if embedding_model was used.\n",
    "- representative_docs_\tThe representative documents for each topic if HDBSCAN is used.\n",
    "\n",
    "\n",
    "For example, to access the predicted topics for the first 10 documents, we simply run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.topics_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.topic_representations_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7dc352",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29063230",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities\n",
    "topic_model.visualize_distribution(probs[80], min_probability=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00da6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.get_topic(topic_model.get_topic_freq().iloc[1].Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c152981",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "topic_model.visualize_barchart(top_n_topics=11, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf981d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first word in a topic fits best, how many more are also good (elbow method?)\n",
    "topic_model.visualize_term_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e832d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_term_rank(log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics(top_n_topics=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c236a",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523d45c",
   "metadata": {},
   "source": [
    "#### KMeans\n",
    "- https://medium.com/nerd-for-tech/extractive-text-summarization-using-sentence-transformer-and-kmeans-clustering-algorithm-e942a6b33860\n",
    "- https://colab.research.google.com/drive/1ClTYut039t-LDtlcd-oQAdXWgcsSGTw9?usp=sharing#scrollTo=pGSwzo0Gjw6R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c912130",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS=3\n",
    "iterations=25\n",
    "X = np.array(data['embeddings'].tolist())\n",
    "kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        repeats=iterations,avoid_empty_clusters=True)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cluster']=pd.Series(assigned_clusters, index=data.index)\n",
    "data['centroid']=data['cluster'].apply(lambda x: kclusterer.means()[x])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11662567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distance between a sentence and a centroid\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def distance_from_centroid(row):\n",
    "    #type of emb and centroid is different, hence using tolist below\n",
    "    return distance_matrix([row['embeddings']], [row['centroid'].tolist()])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['distance_from_centroid'] = data.apply(distance_from_centroid, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f840c",
   "metadata": {},
   "source": [
    "Insert chat here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700244b7",
   "metadata": {},
   "source": [
    "Create a summary:\n",
    "\n",
    "1. Group sentences based on the cluster column.\n",
    "2. Sort the group in ascending order based on the distance_from_centroid column and select the first row (sentence having least distance from the mean)\n",
    "3. Sort the sentences based on their sequence in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ' '.join(data.sort_values('distance_from_centroid',ascending = True).groupby('cluster').head(1).sort_index()['sentence'].tolist())\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fe71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "cluster_model = KMeans(n_clusters=5)\n",
    "topic_model = BERTopic(hdbscan_model=cluster_model).fit(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129d2ba",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb398c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(min_cluster_size=3, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(hdbscan_model=hdbscan_model).fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520633b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625951b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise clusters\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdfe77",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b260d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "topic_model.visualize_hierarchy(top_n_topics=20, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba8007",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "to extract the important words that form a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373bb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5383ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(ngram_range=(2, 2)) # , stop_words=\"danish\"\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model).fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71765b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map - dependencies\n",
    "topic_model.visualize_heatmap(n_clusters=3, top_n_topics=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50cf49",
   "metadata": {},
   "source": [
    "## Dynamic Topics\n",
    "Trump Tweeter example: https://colab.research.google.com/drive/1un8ooI-7ZNlRoK0maVkYhmNRl0XGK88f?usp=sharing#scrollTo=1dR2ckNK782p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=True, \n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b765afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics over time\n",
    "# topics_over_time = topic_model.topics_over_time(abstracts, topics, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20, width=900, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec8c58f",
   "metadata": {},
   "source": [
    "### Topic per Class\n",
    "https://maartengr.github.io/BERTopic/getting_started/topicsperclass/topicsperclass.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cf56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))\n",
    "docs = data[\"data\"]\n",
    "targets = data[\"target\"]\n",
    "target_names = data[\"target_names\"]\n",
    "classes = [data[\"target_names\"][i] for i in data[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d23f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic per class (arxiv example)\n",
    "topics_per_class = topic_model.topics_per_class(abstracts, topics, classes=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=10, width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261d056",
   "metadata": {},
   "source": [
    "### c-TF-IDF\n",
    "for topics representation, create single docs for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3467781",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9178c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularisation and importance\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction\n",
    "for i in range(20):\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(tf_idf.T)\n",
    "    np.fill_diagonal(similarities, 0)\n",
    "\n",
    "    # Extract label to merge into and from where\n",
    "    topic_sizes = docs_df.groupby(['Topic']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
    "    topic_to_merge = topic_sizes.iloc[-1].Topic\n",
    "    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "\n",
    "    # Adjust topics\n",
    "    docs_df.loc[docs_df.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
    "    old_topics = docs_df.sort_values(\"Topic\").Topic.unique()\n",
    "    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
    "    docs_df.Topic = docs_df.Topic.map(map_topics)\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "    # Calculate new topic words\n",
    "    m = len(data)\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f04d3",
   "metadata": {},
   "source": [
    "upper was here: https://www.maartengrootendorst.com/blog/bertopic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37002f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "topic_model = BERTopic(ctfidf_model=ctfidf_model).fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two measures for word selection: bm25_weighting and reduce_frequent_word\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)\n",
    "topic_model = BERTopic(ctfidf_model=ctfidf_model )\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "topic_model = BERTopic(ctfidf_model=ctfidf_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d4495",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1ClTYut039t-LDtlcd-oQAdXWgcsSGTw9?usp=sharing#scrollTo=1NVY1fF0krI8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF from https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#custom-embeddings\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF sparse matrix\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "embeddings = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Train our topic model using TF-IDF vectors\n",
    "topic_model = BERTopic(stop_words=\"english\")\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834b999",
   "metadata": {},
   "source": [
    "### Update Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(docs, n_gram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f14fe",
   "metadata": {},
   "source": [
    "### Reduce Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.reduce_topics(docs, nr_topics=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d48e5a",
   "metadata": {},
   "source": [
    "### Search Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f417d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"database\", top_n=5); similar_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(71)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f07d28",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2421dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "topic_model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "my_model = BERTopic.load(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7de1b",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing#scrollTo=Eh5qp58Hp7Ua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3d60e",
   "metadata": {},
   "source": [
    "More embedding types: https://colab.research.google.com/drive/18arPPe50szvcCp_Y6xS56H2tY0m-RLqv?usp=sharing#scrollTo=Khqzs5D0Ejda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15447ba2",
   "metadata": {},
   "source": [
    "Semi-supervised topic modelling: https://colab.research.google.com/drive/1bxizKzv5vfxJEB29sntU__ZC7PBSIPaQ?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dc940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
