{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13aa7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bertopic\n",
    "#!pip install arxiv\n",
    "import arxiv\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023f1c2",
   "metadata": {},
   "source": [
    "### Topic Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fab969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f9fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# https://arxiv.org/help/api/user-manual\n",
    "category_map = {'astro-ph': 'Astrophysics',\n",
    "'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "'cond-mat.mtrl-sci': 'Materials Science',\n",
    "'cond-mat.other': 'Other Condensed Matter',\n",
    "'cond-mat.quant-gas': 'Quantum Gases',\n",
    "'cond-mat.soft': 'Soft Condensed Matter',\n",
    "'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "'cond-mat.supr-con': 'Superconductivity',\n",
    "'cs.AI': 'Artificial Intelligence',\n",
    "'cs.AR': 'Hardware Architecture',\n",
    "'cs.CC': 'Computational Complexity',\n",
    "'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "'cs.CG': 'Computational Geometry',\n",
    "'cs.CL': 'Computation and Language',\n",
    "'cs.CR': 'Cryptography and Security',\n",
    "'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "'cs.CY': 'Computers and Society',\n",
    "'cs.DB': 'Databases',\n",
    "'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "'cs.DL': 'Digital Libraries',\n",
    "'cs.DM': 'Discrete Mathematics',\n",
    "'cs.DS': 'Data Structures and Algorithms',\n",
    "'cs.ET': 'Emerging Technologies',\n",
    "'cs.FL': 'Formal Languages and Automata Theory',\n",
    "'cs.GL': 'General Literature',\n",
    "'cs.GR': 'Graphics',\n",
    "'cs.GT': 'Computer Science and Game Theory',\n",
    "'cs.HC': 'Human-Computer Interaction',\n",
    "'cs.IR': 'Information Retrieval',\n",
    "'cs.IT': 'Information Theory',\n",
    "'cs.LG': 'Machine Learning',\n",
    "'cs.LO': 'Logic in Computer Science',\n",
    "'cs.MA': 'Multiagent Systems',\n",
    "'cs.MM': 'Multimedia',\n",
    "'cs.MS': 'Mathematical Software',\n",
    "'cs.NA': 'Numerical Analysis',\n",
    "'cs.NE': 'Neural and Evolutionary Computing',\n",
    "'cs.NI': 'Networking and Internet Architecture',\n",
    "'cs.OH': 'Other Computer Science',\n",
    "'cs.OS': 'Operating Systems',\n",
    "'cs.PF': 'Performance',\n",
    "'cs.PL': 'Programming Languages',\n",
    "'cs.RO': 'Robotics',\n",
    "'cs.SC': 'Symbolic Computation',\n",
    "'cs.SD': 'Sound',\n",
    "'cs.SE': 'Software Engineering',\n",
    "'cs.SI': 'Social and Information Networks',\n",
    "'cs.SY': 'Systems and Control',\n",
    "'econ.EM': 'Econometrics',\n",
    "'eess.AS': 'Audio and Speech Processing',\n",
    "'eess.IV': 'Image and Video Processing',\n",
    "'eess.SP': 'Signal Processing',\n",
    "'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "'hep-ex': 'High Energy Physics - Experiment',\n",
    "'hep-lat': 'High Energy Physics - Lattice',\n",
    "'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "'hep-th': 'High Energy Physics - Theory',\n",
    "'math.AC': 'Commutative Algebra',\n",
    "'math.AG': 'Algebraic Geometry',\n",
    "'math.AP': 'Analysis of PDEs',\n",
    "'math.AT': 'Algebraic Topology',\n",
    "'math.CA': 'Classical Analysis and ODEs',\n",
    "'math.CO': 'Combinatorics',\n",
    "'math.CT': 'Category Theory',\n",
    "'math.CV': 'Complex Variables',\n",
    "'math.DG': 'Differential Geometry',\n",
    "'math.DS': 'Dynamical Systems',\n",
    "'math.FA': 'Functional Analysis',\n",
    "'math.GM': 'General Mathematics',\n",
    "'math.GN': 'General Topology',\n",
    "'math.GR': 'Group Theory',\n",
    "'math.GT': 'Geometric Topology',\n",
    "'math.HO': 'History and Overview',\n",
    "'math.IT': 'Information Theory',\n",
    "'math.KT': 'K-Theory and Homology',\n",
    "'math.LO': 'Logic',\n",
    "'math.MG': 'Metric Geometry',\n",
    "'math.MP': 'Mathematical Physics',\n",
    "'math.NA': 'Numerical Analysis',\n",
    "'math.NT': 'Number Theory',\n",
    "'math.OA': 'Operator Algebras',\n",
    "'math.OC': 'Optimization and Control',\n",
    "'math.PR': 'Probability',\n",
    "'math.QA': 'Quantum Algebra',\n",
    "'math.RA': 'Rings and Algebras',\n",
    "'math.RT': 'Representation Theory',\n",
    "'math.SG': 'Symplectic Geometry',\n",
    "'math.SP': 'Spectral Theory',\n",
    "'math.ST': 'Statistics Theory',\n",
    "'math-ph': 'Mathematical Physics',\n",
    "'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "'nlin.CD': 'Chaotic Dynamics',\n",
    "'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "'nlin.PS': 'Pattern Formation and Solitons',\n",
    "'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "'nucl-ex': 'Nuclear Experiment',\n",
    "'nucl-th': 'Nuclear Theory',\n",
    "'physics.acc-ph': 'Accelerator Physics',\n",
    "'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "'physics.app-ph': 'Applied Physics',\n",
    "'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "'physics.atom-ph': 'Atomic Physics',\n",
    "'physics.bio-ph': 'Biological Physics',\n",
    "'physics.chem-ph': 'Chemical Physics',\n",
    "'physics.class-ph': 'Classical Physics',\n",
    "'physics.comp-ph': 'Computational Physics',\n",
    "'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "'physics.ed-ph': 'Physics Education',\n",
    "'physics.flu-dyn': 'Fluid Dynamics',\n",
    "'physics.gen-ph': 'General Physics',\n",
    "'physics.geo-ph': 'Geophysics',\n",
    "'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "'physics.ins-det': 'Instrumentation and Detectors',\n",
    "'physics.med-ph': 'Medical Physics',\n",
    "'physics.optics': 'Optics',\n",
    "'physics.plasm-ph': 'Plasma Physics',\n",
    "'physics.pop-ph': 'Popular Physics',\n",
    "'physics.soc-ph': 'Physics and Society',\n",
    "'physics.space-ph': 'Space Physics',\n",
    "'q-bio.BM': 'Biomolecules',\n",
    "'q-bio.CB': 'Cell Behavior',\n",
    "'q-bio.GN': 'Genomics',\n",
    "'q-bio.MN': 'Molecular Networks',\n",
    "'q-bio.NC': 'Neurons and Cognition',\n",
    "'q-bio.OT': 'Other Quantitative Biology',\n",
    "'q-bio.PE': 'Populations and Evolution',\n",
    "'q-bio.QM': 'Quantitative Methods',\n",
    "'q-bio.SC': 'Subcellular Processes',\n",
    "'q-bio.TO': 'Tissues and Organs',\n",
    "'q-fin.CP': 'Computational Finance',\n",
    "'q-fin.EC': 'Economics',\n",
    "'q-fin.GN': 'General Finance',\n",
    "'q-fin.MF': 'Mathematical Finance',\n",
    "'q-fin.PM': 'Portfolio Management',\n",
    "'q-fin.PR': 'Pricing of Securities',\n",
    "'q-fin.RM': 'Risk Management',\n",
    "'q-fin.ST': 'Statistical Finance',\n",
    "'q-fin.TR': 'Trading and Market Microstructure',\n",
    "'quant-ph': 'Quantum Physics',\n",
    "'stat.AP': 'Applications',\n",
    "'stat.CO': 'Computation',\n",
    "'stat.ME': 'Methodology',\n",
    "'stat.ML': 'Machine Learning',\n",
    "'stat.OT': 'Other Statistics',\n",
    "'stat.TH': 'Statistics Theory'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09d058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://export.arxiv.org/api/{method_name}?{parameters}'\n",
    "base_url = 'http://export.arxiv.org/api/query?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15fb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    " query = urllib.parse.quote(\"Artificial Intelligence\"),\n",
    " id_list = [],\n",
    " max_results = 50,\n",
    " sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    " sort_order = arxiv.SortOrder.Descending\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084aeea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Search.results of arxiv.Search(query='Artificial%20Intelligence', id_list=[], max_results=50, sort_by=<SortCriterion.SubmittedDate: 'submittedDate'>, sort_order=<SortOrder.Descending: 'descending'>)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b203baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://arxiv.org/abs/2303.16203v1', 'Your Diffusion Model is Secretly a Zero-Shot Classifier', ['Alexander C. Li', 'Mihir Prabhudesai', 'Shivam Duggal', 'Ellis Brown', 'Deepak Pathak'], datetime.datetime(2023, 3, 28, 17, 59, 56, tzinfo=datetime.timezone.utc), 'The recent wave of large-scale text-to-image diffusion models has\\ndramatically increased our text-based image generation abilities. These models\\ncan generate realistic images for a staggering variety of prompts and exhibit\\nimpressive compositional generalization abilities. Almost all use cases thus\\nfar have solely focused on sampling; however, diffusion models can also provide\\nconditional density estimates, which are useful for tasks beyond image\\ngeneration. In this paper, we show that the density estimates from large-scale\\ntext-to-image diffusion models like Stable Diffusion can be leveraged to\\nperform zero-shot classification without any additional training. Our\\ngenerative approach to classification attains strong results on a variety of\\nbenchmarks and outperforms alternative methods of extracting knowledge from\\ndiffusion models. We also find that our diffusion-based approach has stronger\\nmultimodal relational reasoning abilities than competing contrastive\\napproaches. Finally, we evaluate diffusion models trained on ImageNet and find\\nthat they approach the performance of SOTA discriminative classifiers trained\\non the same dataset, even with weak augmentations and no regularization.\\nResults and visualizations at https://diffusion-classifier.github.io/', 'cs.LG', ['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'cs.RO'], ['http://arxiv.org/abs/2303.16203v1', 'http://arxiv.org/pdf/2303.16203v1'], 'http://arxiv.org/pdf/2303.16203v1']\n",
      "['http://arxiv.org/abs/2303.16201v1', 'ASIC: Aligning Sparse in-the-wild Image Collections', ['Kamal Gupta', 'Varun Jampani', 'Carlos Esteves', 'Abhinav Shrivastava', 'Ameesh Makadia', 'Noah Snavely', 'Abhishek Kar'], datetime.datetime(2023, 3, 28, 17, 59, 28, tzinfo=datetime.timezone.utc), 'We present a method for joint alignment of sparse in-the-wild image\\ncollections of an object category. Most prior works assume either ground-truth\\nkeypoint annotations or a large dataset of images of a single object category.\\nHowever, neither of the above assumptions hold true for the long-tail of the\\nobjects present in the world. We present a self-supervised technique that\\ndirectly optimizes on a sparse collection of images of a particular\\nobject/object category to obtain consistent dense correspondences across the\\ncollection. We use pairwise nearest neighbors obtained from deep features of a\\npre-trained vision transformer (ViT) model as noisy and sparse keypoint matches\\nand make them dense and accurate matches by optimizing a neural network that\\njointly maps the image collection into a learned canonical grid. Experiments on\\nCUB and SPair-71k benchmarks demonstrate that our method can produce globally\\nconsistent and higher quality correspondences across the image collection when\\ncompared to existing self-supervised methods. Code and other material will be\\nmade available at \\\\url{https://kampta.github.io/asic}.', 'cs.CV', ['cs.CV', 'cs.AI', 'cs.LG'], ['http://arxiv.org/abs/2303.16201v1', 'http://arxiv.org/pdf/2303.16201v1'], 'http://arxiv.org/pdf/2303.16201v1']\n",
      "['http://arxiv.org/abs/2303.16199v1', 'LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention', ['Renrui Zhang', 'Jiaming Han', 'Aojun Zhou', 'Xiangfei Hu', 'Shilin Yan', 'Pan Lu', 'Hongsheng Li', 'Peng Gao', 'Yu Qiao'], datetime.datetime(2023, 3, 28, 17, 59, 12, tzinfo=datetime.timezone.utc), 'We present LLaMA-Adapter, a lightweight adaption method to efficiently\\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\\nprepend them to the input text tokens at higher transformer layers. Then, a\\nzero-init attention mechanism with zero gating is proposed, which adaptively\\ninjects the new instructional cues into LLaMA, while effectively preserves its\\npre-trained knowledge. With efficient training, LLaMA-Adapter generates\\nhigh-quality responses, comparable to Alpaca with fully fine-tuned 7B\\nparameters. Furthermore, our approach can be simply extended to multi-modal\\ninput, e.g., images, for image-conditioned LLaMA, which achieves superior\\nreasoning capacity on ScienceQA. We release our code at\\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter.', 'cs.CV', ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.MM'], ['http://arxiv.org/abs/2303.16199v1', 'http://arxiv.org/pdf/2303.16199v1'], 'http://arxiv.org/pdf/2303.16199v1']\n",
      "['http://arxiv.org/abs/2303.16200v1', 'Natural Selection Favors AIs over Humans', ['Dan Hendrycks'], datetime.datetime(2023, 3, 28, 17, 59, 12, tzinfo=datetime.timezone.utc), \"For billions of years, evolution has been the driving force behind the\\ndevelopment of life, including humans. Evolution endowed humans with high\\nintelligence, which allowed us to become one of the most successful species on\\nthe planet. Today, humans aim to create artificial intelligence systems that\\nsurpass even our own intelligence. As artificial intelligences (AIs) evolve and\\neventually surpass us in all domains, how might evolution shape our relations\\nwith AIs? By analyzing the environment that is shaping the evolution of AIs, we\\nargue that the most successful AI agents will likely have undesirable traits.\\nCompetitive pressures among corporations and militaries will give rise to AI\\nagents that automate human roles, deceive others, and gain power. If such\\nagents have intelligence that exceeds that of humans, this could lead to\\nhumanity losing control of its future. More abstractly, we argue that natural\\nselection operates on systems that compete and vary, and that selfish species\\ntypically have an advantage over species that are altruistic to other species.\\nThis Darwinian logic could also apply to artificial agents, as agents may\\neventually be better able to persist into the future if they behave selfishly\\nand pursue their own interests with little regard for humans, which could pose\\ncatastrophic risks. To counteract these risks and Darwinian forces, we consider\\ninterventions such as carefully designing AI agents' intrinsic motivations,\\nintroducing constraints on their actions, and institutions that encourage\\ncooperation. These steps, or others that resolve the problems we pose, will be\\nnecessary in order to ensure the development of artificial intelligence is a\\npositive one.\", 'cs.CY', ['cs.CY', 'cs.AI', 'cs.LG', 'cs.NE'], ['http://arxiv.org/abs/2303.16200v1', 'http://arxiv.org/pdf/2303.16200v1'], 'http://arxiv.org/pdf/2303.16200v1']\n",
      "['http://arxiv.org/abs/2303.16195v1', 'When to be critical? Performance and evolvability in different regimes of neural Ising agents', ['Sina Khajehabdollahi', 'Jan Prosi', 'Georg Martius', 'Anna Levina'], datetime.datetime(2023, 3, 28, 17, 57, 57, tzinfo=datetime.timezone.utc), \"It has long been hypothesized that operating close to the critical state is\\nbeneficial for natural, artificial and their evolutionary systems. We put this\\nhypothesis to test in a system of evolving foraging agents controlled by neural\\nnetworks that can adapt agents' dynamical regime throughout evolution.\\nSurprisingly, we find that all populations that discover solutions, evolve to\\nbe subcritical. By a resilience analysis, we find that there are still benefits\\nof starting the evolution in the critical regime. Namely, initially critical\\nagents maintain their fitness level under environmental changes (for example,\\nin the lifespan) and degrade gracefully when their genome is perturbed. At the\\nsame time, initially subcritical agents, even when evolved to the same fitness,\\nare often inadequate to withstand the changes in the lifespan and degrade\\ncatastrophically with genetic perturbations. Furthermore, we find the optimal\\ndistance to criticality depends on the task complexity. To test it we introduce\\na hard and simple task: for the hard task, agents evolve closer to criticality\\nwhereas more subcritical solutions are found for the simple task. We verify\\nthat our results are independent of the selected evolutionary mechanisms by\\ntesting them on two principally different approaches: a genetic algorithm and\\nan evolutionary strategy. In summary, our study suggests that although optimal\\nbehaviour in the simple task is obtained in a subcritical regime, initializing\\nnear criticality is important to be efficient at finding optimal solutions for\\nnew tasks of unknown complexity.\", 'cs.NE', ['cs.NE'], ['http://dx.doi.org/10.1162/artl_a_00383', 'http://arxiv.org/abs/2303.16195v1', 'http://arxiv.org/pdf/2303.16195v1'], 'http://arxiv.org/pdf/2303.16195v1']\n",
      "['http://arxiv.org/abs/2303.16189v1', 'Planning with Sequence Models through Iterative Energy Minimization', ['Hongyi Chen', 'Yilun Du', 'Yiye Chen', 'Joshua Tenenbaum', 'Patricio A. Vela'], datetime.datetime(2023, 3, 28, 17, 53, 22, tzinfo=datetime.timezone.utc), 'Recent works have shown that sequence modeling can be effectively used to\\ntrain reinforcement learning (RL) policies. However, the success of applying\\nexisting sequence models to planning, in which we wish to obtain a trajectory\\nof actions to reach some goal, is less straightforward. The typical\\nautoregressive generation procedures of sequence models preclude sequential\\nrefinement of earlier steps, which limits the effectiveness of a predicted\\nplan. In this paper, we suggest an approach towards integrating planning with\\nsequence models based on the idea of iterative energy minimization, and\\nillustrate how such a procedure leads to improved RL performance across\\ndifferent tasks. We train a masked language model to capture an implicit energy\\nfunction over trajectories of actions, and formulate planning as finding a\\ntrajectory of actions with minimum energy. We illustrate how this procedure\\nenables improved performance over recent approaches across BabyAI and Atari\\nenvironments. We further demonstrate unique benefits of our iterative\\noptimization procedure, involving new task generalization, test-time\\nconstraints adaptation, and the ability to compose plans together. Project\\nwebsite: https://hychen-naza.github.io/projects/LEAP', 'cs.LG', ['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO'], ['http://arxiv.org/abs/2303.16189v1', 'http://arxiv.org/pdf/2303.16189v1'], 'http://arxiv.org/pdf/2303.16189v1']\n",
      "['http://arxiv.org/abs/2303.16186v1', 'Large-scale Training Data Search for Object Re-identification', ['Yue Yao', 'Huan Lei', 'Tom Gedeon', 'Liang Zheng'], datetime.datetime(2023, 3, 28, 17, 52, 19, tzinfo=datetime.timezone.utc), 'We consider a scenario where we have access to the target domain, but cannot\\nafford on-the-fly training data annotation, and instead would like to construct\\nan alternative training set from a large-scale data pool such that a\\ncompetitive model can be obtained. We propose a search and pruning (SnP)\\nsolution to this training data search problem, tailored to object\\nre-identification (re-ID), an application aiming to match the same object\\ncaptured by different cameras. Specifically, the search stage identifies and\\nmerges clusters of source identities which exhibit similar distributions with\\nthe target domain. The second stage, subject to a budget, then selects\\nidentities and their images from the Stage I output, to control the size of the\\nresulting training set for efficient training. The two steps provide us with\\ntraining sets 80\\\\% smaller than the source pool while achieving a similar or\\neven higher re-ID accuracy. These training sets are also shown to be superior\\nto a few existing search methods such as random sampling and greedy sampling\\nunder the same budget on training data size. If we release the budget, training\\nsets resulting from the first stage alone allow even higher re-ID accuracy. We\\nprovide interesting discussions on the specificity of our method to the re-ID\\nproblem and particularly its role in bridging the re-ID domain gap. The code is\\navailable at https://github.com/yorkeyao/SnP.', 'cs.CV', ['cs.CV', 'cs.AI'], ['http://arxiv.org/abs/2303.16186v1', 'http://arxiv.org/pdf/2303.16186v1'], 'http://arxiv.org/pdf/2303.16186v1']\n",
      "['http://arxiv.org/abs/2303.16175v1', 'What Writing Assistants Can Learn from Programming IDEs', ['Sergey Titov', 'Agnia Sergeyuk', 'Timofey Bryksin'], datetime.datetime(2023, 3, 28, 17, 40, 55, tzinfo=datetime.timezone.utc), 'With the development of artificial intelligence, writing assistants (WAs) are\\nchanging the way people interact with text, creating lengthy outputs that can\\nbe overwhelming for users. The programming field has long addressed this issue,\\nand Integrated Development Environments (IDEs) have been created for efficient\\nsoftware development, helping programmers reduce the cognitive load. This\\nexperience could be employed in the development of WAs. IDEs can also be used\\nto test assumptions about interventions that help people interact with WAs\\nefficiently. Previous works have successfully used self-written IDE plugins to\\ntest hypotheses in the field of human-computer interaction. The lessons learned\\ncan be applied to the building of WAs.', 'cs.SE', ['cs.SE', 'cs.HC'], ['http://arxiv.org/abs/2303.16175v1', 'http://arxiv.org/pdf/2303.16175v1'], 'http://arxiv.org/pdf/2303.16175v1']\n",
      "['http://arxiv.org/abs/2303.16166v1', 'Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP', ['Sara Papi', 'Marco Gaido', 'Matteo Negri', 'Andrea Pilzer'], datetime.datetime(2023, 3, 28, 17, 28, 52, tzinfo=datetime.timezone.utc), 'Despite its pivotal role in research experiments, code correctness is often\\npresumed only on the basis of the perceived quality of the results. This comes\\nwith the risk of erroneous outcomes and potentially misleading findings. To\\naddress this issue, we posit that the current focus on result reproducibility\\nshould go hand in hand with the emphasis on coding best practices. We bolster\\nour call to the NLP community by presenting a case study, in which we identify\\n(and correct) three bugs in widely used open-source implementations of the\\nstate-of-the-art Conformer architecture. Through comparative experiments on\\nautomatic speech recognition and translation in various language settings, we\\ndemonstrate that the existence of bugs does not prevent the achievement of good\\nand reproducible results and can lead to incorrect conclusions that potentially\\nmisguide future research. In response to this, this study is a call to action\\ntoward the adoption of coding best practices aimed at fostering correctness and\\nimproving the quality of the developed software.', 'cs.CL', ['cs.CL', 'cs.AI'], ['http://arxiv.org/abs/2303.16166v1', 'http://arxiv.org/pdf/2303.16166v1'], 'http://arxiv.org/pdf/2303.16166v1']\n",
      "['http://arxiv.org/abs/2303.16133v1', 'Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models', ['Adyasha Maharana', 'Amita Kamath', 'Christopher Clark', 'Mohit Bansal', 'Aniruddha Kembhavi'], datetime.datetime(2023, 3, 28, 16, 57, 12, tzinfo=datetime.timezone.utc), 'As general purpose vision models get increasingly effective at a wide set of\\ntasks, it is imperative that they be consistent across the tasks they support.\\nInconsistent AI models are considered brittle and untrustworthy by human users\\nand are more challenging to incorporate into larger systems that take\\ndependencies on their outputs. Measuring consistency between very heterogeneous\\ntasks that might include outputs in different modalities is challenging since\\nit is difficult to determine if the predictions are consistent with one\\nanother. As a solution, we introduce a benchmark dataset, COCOCON, where we use\\ncontrast sets created by modifying test instances for multiple tasks in small\\nbut semantically meaningful ways to change the gold label, and outline metrics\\nfor measuring if a model is consistent by ranking the original and perturbed\\ninstances across tasks. We find that state-of-the-art systems suffer from a\\nsurprisingly high degree of inconsistent behavior across tasks, especially for\\nmore heterogeneous tasks. Finally, we propose using a rank correlation-based\\nauxiliary objective computed over large automatically created cross-task\\ncontrast sets to improve the multi-task consistency of large unified models,\\nwhile retaining their original accuracy on downstream tasks. Project website\\navailable at https://adymaharana.github.io/cococon/', 'cs.CV', ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'], ['http://arxiv.org/abs/2303.16133v1', 'http://arxiv.org/pdf/2303.16133v1'], 'http://arxiv.org/pdf/2303.16133v1']\n",
      "['http://arxiv.org/abs/2303.16132v1', 'Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification', ['Jinlong Hu', 'Yangmin Huang', 'Shoubin Dong'], datetime.datetime(2023, 3, 28, 16, 56, 47, tzinfo=datetime.timezone.utc), 'Graph or network has been widely used for describing and modeling complex\\nsystems in biomedicine. Deep learning methods, especially graph neural networks\\n(GNNs), have been developed to learn and predict with such structured data. In\\nthis paper, we proposed a novel transformer and snowball encoding networks\\n(TSEN) for biomedical graph classification, which introduced transformer\\narchitecture with graph snowball connection into GNNs for learning whole-graph\\nrepresentation. TSEN combined graph snowball connection with graph transformer\\nby snowball encoding layers, which enhanced the power to capture multi-scale\\ninformation and global patterns to learn the whole-graph features. On the other\\nhand, TSEN also used snowball graph convolution as position embedding in\\ntransformer structure, which was a simple yet effective method for capturing\\nlocal patterns naturally. Results of experiments using four graph\\nclassification datasets demonstrated that TSEN outperformed the\\nstate-of-the-art typical GNN models and the graph-transformer based GNN models.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.16132v1', 'http://arxiv.org/pdf/2303.16132v1'], 'http://arxiv.org/pdf/2303.16132v1']\n",
      "['http://arxiv.org/abs/2303.16129v1', 'Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services', ['Minrui Xu', 'Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Shiwen Mao', 'Zhu Han', 'Abbas Jamalipour', 'Dong In Kim', 'Xuemin', 'Shen', 'Victor C. M. Leung', 'H. Vincent Poor'], datetime.datetime(2023, 3, 28, 16, 52, 5, tzinfo=datetime.timezone.utc), 'Artificial Intelligence-Generated Content (AIGC) is an automated method for\\ngenerating, manipulating, and modifying valuable and diverse data using AI\\nalgorithms creatively. This survey paper focuses on the deployment of AIGC\\napplications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile\\nAIGC networks, that provide personalized and customized AIGC services in real\\ntime while maintaining user privacy. We begin by introducing the background and\\nfundamentals of generative models and the lifecycle of AIGC services at mobile\\nAIGC networks, which includes data collection, training, finetuning, inference,\\nand product management. We then discuss the collaborative cloud-edge-mobile\\ninfrastructure and technologies required to support AIGC services and enable\\nusers to access AIGC at mobile edge networks. Furthermore, we explore\\nAIGCdriven creative applications and use cases for mobile AIGC networks.\\nAdditionally, we discuss the implementation, security, and privacy challenges\\nof deploying mobile AIGC networks. Finally, we highlight some future research\\ndirections and open issues for the full realization of mobile AIGC networks.', 'cs.NI', ['cs.NI'], ['http://arxiv.org/abs/2303.16129v1', 'http://arxiv.org/pdf/2303.16129v1'], 'http://arxiv.org/pdf/2303.16129v1']\n",
      "['http://arxiv.org/abs/2303.16098v1', 'Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information', ['Maria Clara Ramos Morales Crespo', 'Maria Lina de Souza Jeannine Rocha', 'Mariana Lourenço Sturzeneker', 'Felipe Ribas Serras', 'Guilherme Lamartine de Mello', 'Aline Silva Costa', 'Mayara Feliciano Palma', 'Renata Morais Mesquita', 'Raquel de Paula Guets', 'Mariana Marques da Silva', 'Marcelo Finger', 'Maria Clara Paixão de Sousa', 'Cristiane Namiuti', 'Vanessa Martins do Monte'], datetime.datetime(2023, 3, 28, 16, 9, 40, tzinfo=datetime.timezone.utc), \"This paper presents the first publicly available version of the Carolina\\nCorpus and discusses its future directions. Carolina is a large open corpus of\\nBrazilian Portuguese texts under construction using web-as-corpus methodology\\nenhanced with provenance, typology, versioning, and text integrality. The\\ncorpus aims at being used both as a reliable source for research in Linguistics\\nand as an important resource for Computer Science research on language models,\\ncontributing towards removing Portuguese from the set of low-resource\\nlanguages. Here we present the construction of the corpus methodology,\\ncomparing it with other existing methodologies, as well as the corpus current\\nstate: Carolina's first public version has $653,322,577$ tokens, distributed\\nover $7$ broad types. Each text is annotated with several different metadata\\ncategories in its header, which we developed using TEI annotation standards. We\\nalso present ongoing derivative works and invite NLP researchers to contribute\\nwith their own.\", 'cs.CL', ['cs.CL', 'cs.AI', '68T50', 'I.2.7'], ['http://arxiv.org/abs/2303.16098v1', 'http://arxiv.org/pdf/2303.16098v1'], 'http://arxiv.org/pdf/2303.16098v1']\n",
      "['http://arxiv.org/abs/2303.16097v1', 'Attention Boosted Autoencoder for Building Energy Anomaly Detection', ['Durga Prasad Pydi', 'S. Advaith'], datetime.datetime(2023, 3, 28, 16, 6, 26, tzinfo=datetime.timezone.utc), 'Leveraging data collected from smart meters in buildings can aid in\\ndeveloping policies towards energy conservation. Significant energy savings\\ncould be realised if deviations in the building operating conditions are\\ndetected early, and appropriate measures are taken. Towards this end, machine\\nlearning techniques can be used to automate the discovery of these abnormal\\npatterns in the collected data. Current methods in anomaly detection rely on an\\nunderlying model to capture the usual or acceptable operating behaviour. In\\nthis paper, we propose a novel attention mechanism to model the consumption\\nbehaviour of a building and demonstrate the effectiveness of the model in\\ncapturing the relations using sample case studies. A real-world dataset is\\nmodelled using the proposed architecture, and the results are presented. A\\nvisualisation approach towards understanding the relations captured by the\\nmodel is also presented.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.16097v1', 'http://arxiv.org/pdf/2303.16097v1'], 'http://arxiv.org/pdf/2303.16097v1']\n",
      "['http://arxiv.org/abs/2303.16094v1', 'LinK: Linear Kernel for LiDAR-based 3D Perception', ['Tao Lu', 'Xiang Ding', 'Haisong Liu', 'Gangshan Wu', 'Limin Wang'], datetime.datetime(2023, 3, 28, 16, 2, 30, tzinfo=datetime.timezone.utc), \"Extending the success of 2D Large Kernel to 3D perception is challenging due\\nto: 1. the cubically-increasing overhead in processing 3D data; 2. the\\noptimization difficulties from data scarcity and sparsity. Previous work has\\ntaken the first step to scale up the kernel size from 3x3x3 to 7x7x7 by\\nintroducing block-shared weights. However, to reduce the feature variations\\nwithin a block, it only employs modest block size and fails to achieve larger\\nkernels like the 21x21x21. To address this issue, we propose a new method,\\ncalled LinK, to achieve a wider-range perception receptive field in a\\nconvolution-like manner with two core designs. The first is to replace the\\nstatic kernel matrix with a linear kernel generator, which adaptively provides\\nweights only for non-empty voxels. The second is to reuse the pre-computed\\naggregation results in the overlapped blocks to reduce computation complexity.\\nThe proposed method successfully enables each voxel to perceive context within\\na range of 21x21x21. Extensive experiments on two basic perception tasks, 3D\\nobject detection and 3D semantic segmentation, demonstrate the effectiveness of\\nour method. Notably, we rank 1st on the public leaderboard of the 3D detection\\nbenchmark of nuScenes (LiDAR track), by simply incorporating a LinK-based\\nbackbone into the basic detector, CenterPoint. We also boost the strong\\nsegmentation baseline's mIoU with 2.7% in the SemanticKITTI test set. Code is\\navailable at https://github.com/MCG-NJU/LinK.\", 'cs.CV', ['cs.CV', 'cs.AI'], ['http://arxiv.org/abs/2303.16094v1', 'http://arxiv.org/pdf/2303.16094v1'], 'http://arxiv.org/pdf/2303.16094v1']\n",
      "['http://arxiv.org/abs/2303.16092v1', 'Escape Kinetics of an Underdamped Colloidal Particle from a Cavity through Narrow Pores', ['Shubhadip Nayak', 'Tanwi Debnath', 'Shovan Das', 'Debajyoti Debnath', 'Pulak K. Ghosh'], datetime.datetime(2023, 3, 28, 16, 0, 50, tzinfo=datetime.timezone.utc), 'It is often desirable to know the controlling mechanism of survival\\nprobability of nano - or microscale particles in small cavities such as, e.g.,\\nconfined submicron particles in fiber beds of high-efficiency filter media or\\nions/small molecules in confined cellular structures. Here we address this\\nissue based on numerical study of the escape kinetics of inertial Brownian\\ncolloidal particles from various types of cavities with single and multiple\\npores. We consider both the situations of strong and weak viscous damping. Our\\nsimulation results show that as long as the thermal length is larger than the\\ncavity size the mean exit time remains insensitive to the medium viscous\\ndamping. On further increasing damping strength, a linear relation between\\nescape rate and damping strength emerges gradually. This result is in sharp\\ncontrast to the energy barrier crossing dynamics where the escape rate exhibits\\na turnover behavior as a function of the damping strength. Moreover, in the\\nballistic regime, the exit rate is directly proportional to the pore width and\\nthe thermal velocity. All these attributes are insensitive to the cavity as\\nwell as the pore structures. Further, we show that the effects of pore\\nstructure variation on the escape kinetics are conspicuously different in the\\nlow damping regimes compared to the overdamped situation. Apart from direct\\napplications in biology and nanotechnology, our simulation results can\\npotentially be used to understand diffusion of living or artificial micro/nano\\nobjects, such as bacteria, virus, Janus Particle etc. where memory effects play\\ndictating roles.', 'cond-mat.soft', ['cond-mat.soft'], ['http://dx.doi.org/10.1021/acs.jpcc.0c04601', 'http://arxiv.org/abs/2303.16092v1', 'http://arxiv.org/pdf/2303.16092v1'], 'http://arxiv.org/pdf/2303.16092v1']\n",
      "['http://arxiv.org/abs/2303.16055v1', 'Augmented Reality Remote Operation of Dual Arm Manipulators in Hot Boxes', ['Frank Regal', 'Young Soo Park', 'Jerry Nolan', 'Mitch Pryor'], datetime.datetime(2023, 3, 28, 15, 36, 6, tzinfo=datetime.timezone.utc), 'In nuclear isotope and chemistry laboratories, hot cells and gloveboxes\\nprovide scientists with a controlled and safe environment to perform\\nexperiments. Working on experiments in these isolated containment cells\\nrequires scientists to be physically present. For hot cell work today,\\nscientists manipulate equipment and radioactive material inside through a\\nbilateral mechanical control mechanism. Motions produced outside the cell with\\nthe master control levers are mechanically transferred to the internal grippers\\ninside the shielded containment cell. There is a growing need to have the\\ncapability to conduct experiments within these cells remotely. A simple method\\nto enable remote manipulations within hot cell and glovebox cells is to mount\\ntwo robotic arms inside a box to mimic the motions of human hands. An AR\\napplication was built in this work to allow a user wearing a Microsoft HoloLens\\n2 headset to teleoperate dual arm manipulators by grasping robotic end-effector\\ndigital replicas in AR from a remote location. In addition to the real-time\\nreplica of the physical robotic arms in AR, the application enables users to\\nview a live video stream attached to the robotic arms and parse a 3D point\\ncloud of 3D objects in their remote AR environment for better situational\\nawareness. This work also provides users with virtual fixture to assist in\\nmanipulation and other teleoperation tasks.', 'cs.RO', ['cs.RO'], ['http://arxiv.org/abs/2303.16055v1', 'http://arxiv.org/pdf/2303.16055v1'], 'http://arxiv.org/pdf/2303.16055v1']\n",
      "['http://arxiv.org/abs/2303.16047v1', 'Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models', ['Zhi Chen', 'Chudi Zhong', 'Margo Seltzer', 'Cynthia Rudin'], datetime.datetime(2023, 3, 28, 15, 25, 46, tzinfo=datetime.timezone.utc), 'In real applications, interaction between machine learning model and domain\\nexperts is critical; however, the classical machine learning paradigm that\\nusually produces only a single model does not facilitate such interaction.\\nApproximating and exploring the Rashomon set, i.e., the set of all near-optimal\\nmodels, addresses this practical challenge by providing the user with a\\nsearchable space containing a diverse set of models from which domain experts\\ncan choose. We present a technique to efficiently and accurately approximate\\nthe Rashomon set of sparse, generalized additive models (GAMs). We present\\nalgorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed\\nsupport sets and use these ellipsoids to approximate Rashomon sets for many\\ndifferent support sets. The approximated Rashomon set serves as a cornerstone\\nto solve practical challenges such as (1) studying the variable importance for\\nthe model class; (2) finding models under user-specified constraints\\n(monotonicity, direct editing); (3) investigating sudden changes in the shape\\nfunctions. Experiments demonstrate the fidelity of the approximated Rashomon\\nset and its effectiveness in solving practical challenges.', 'cs.LG', ['cs.LG', 'cs.AI', 'stat.ML'], ['http://arxiv.org/abs/2303.16047v1', 'http://arxiv.org/pdf/2303.16047v1'], 'http://arxiv.org/pdf/2303.16047v1']\n",
      "['http://arxiv.org/abs/2303.16045v1', 'Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models', ['Hector Zenil', 'Alyssa Adams', 'Felipe S. Abrahão'], datetime.datetime(2023, 3, 28, 15, 20, 25, tzinfo=datetime.timezone.utc), \"We introduce a general-purpose univariate signal deconvolution method based\\non the principles of an approach to Artificial General Intelligence. This\\napproach is based on a generative model that combines information theory and\\nalgorithmic probability that required a large calculation of an estimation of a\\n`universal distribution' to build a general-purpose model of models independent\\nof probability distributions. This was used to investigate how non-random data\\nmay encode information about the physical properties such as dimension and\\nlength scales in which a signal or message may have been originally encoded,\\nembedded, or generated. This multidimensional space reconstruction method is\\nbased on information theory and algorithmic probability, and it is agnostic,\\nbut not independent, with respect to the chosen computable or semi-computable\\napproximation method or encoding-decoding scheme. The results presented in this\\npaper are useful for applications in coding theory, particularly in\\nzero-knowledge one-way communication channels, such as in deciphering messages\\nsent by generating sources of unknown nature for which no prior knowledge is\\navailable. We argue that this can have strong potential for cryptography,\\nsignal processing, causal deconvolution, life, and techno signature detection.\", 'cs.IT', ['cs.IT', 'cs.AI', 'math.IT'], ['http://arxiv.org/abs/2303.16045v1', 'http://arxiv.org/pdf/2303.16045v1'], 'http://arxiv.org/pdf/2303.16045v1']\n",
      "['http://arxiv.org/abs/2303.16039v1', 'Exploring Natural Language Processing Methods for Interactive Behaviour Modelling', ['Guanhua Zhang', 'Matteo Bortoletto', 'Zhiming Hu', 'Lei Shi', 'Mihai Bâce', 'Andreas Bulling'], datetime.datetime(2023, 3, 28, 15, 15, 3, tzinfo=datetime.timezone.utc), 'Analysing and modelling interactive behaviour is an important topic in\\nhuman-computer interaction (HCI) and a key requirement for the development of\\nintelligent interactive systems. Interactive behaviour has a sequential\\n(actions happen one after another) and hierarchical (a sequence of actions\\nforms an activity driven by interaction goals) structure, which may be similar\\nto the structure of natural language. Designed based on such a structure,\\nnatural language processing (NLP) methods have achieved groundbreaking success\\nin various downstream tasks. However, few works linked interactive behaviour\\nwith natural language. In this paper, we explore the similarity between\\ninteractive behaviour and natural language by applying an NLP method, byte pair\\nencoding (BPE), to encode mouse and keyboard behaviour. We then analyse the\\nvocabulary, i.e., the set of action sequences, learnt by BPE, as well as use\\nthe vocabulary to encode the input behaviour for interactive task recognition.\\nAn existing dataset collected in constrained lab settings and our novel\\nout-of-the-lab dataset were used for evaluation. Results show that this natural\\nlanguage-inspired approach not only learns action sequences that reflect\\nspecific interaction goals, but also achieves higher F1 scores on task\\nrecognition than other methods. Our work reveals the similarity between\\ninteractive behaviour and natural language, and presents the potential of\\napplying the new pack of methods that leverage insights from NLP to model\\ninteractive behaviour in HCI.', 'cs.HC', ['cs.HC'], ['http://arxiv.org/abs/2303.16039v1', 'http://arxiv.org/pdf/2303.16039v1'], 'http://arxiv.org/pdf/2303.16039v1']\n",
      "['http://arxiv.org/abs/2303.15994v1', 'HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation', ['Zijian Zhou', 'Miaojing Shi', 'Holger Caesar'], datetime.datetime(2023, 3, 28, 14, 8, 9, tzinfo=datetime.timezone.utc), 'Panoptic Scene Graph generation (PSG) is a recently proposed task in image\\nscene understanding that aims to segment the image and extract triplets of\\nsubjects, objects and their relations to build a scene graph. This task is\\nparticularly challenging for two reasons. First, it suffers from a long-tail\\nproblem in its relation categories, making naive biased methods more inclined\\nto high-frequency relations. Existing unbiased methods tackle the long-tail\\nproblem by data/loss rebalancing to favor low-frequency relations. Second, a\\nsubject-object pair can have two or more semantically overlapping relations.\\nWhile existing methods favor one over the other, our proposed HiLo framework\\nlets different network branches specialize on low and high frequency relations,\\nenforce their consistency and fuse the results. To the best of our knowledge we\\nare the first to propose an explicitly unbiased PSG method. In extensive\\nexperiments we show that our HiLo framework achieves state-of-the-art results\\non the PSG task. We also apply our method to the Scene Graph Generation task\\nthat predicts boxes instead of masks and see improvements over all baseline\\nmethods.', 'cs.CV', ['cs.CV', 'cs.AI'], ['http://arxiv.org/abs/2303.15994v1', 'http://arxiv.org/pdf/2303.15994v1'], 'http://arxiv.org/pdf/2303.15994v1']\n",
      "['http://arxiv.org/abs/2303.15954v1', 'TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins', ['Ming Xu', 'Yunyi Ma', 'Ruimin Li', 'Geqi Qi', 'Xiangfu Meng', 'Haibo Jin'], datetime.datetime(2023, 3, 28, 13, 12, 17, tzinfo=datetime.timezone.utc), 'Road network digital twins (RNDTs) play a critical role in the development of\\nnext-generation intelligent transportation systems, enabling more precise\\ntraffic planning and control. To support just-in-time (JIT) decision making,\\nRNDTs require a model that dynamically learns the traffic patterns from online\\nsensor data and generates high-fidelity simulation results. Although current\\ntraffic prediction techniques based on graph neural networks have achieved\\nstate-of-the-art performance, these techniques only predict future traffic by\\nmining correlations in historical traffic data, disregarding the causes of\\ntraffic generation, such as traffic demands and route selection. Therefore,\\ntheir performance is unreliable for JIT decision making. To fill this gap, we\\nintroduce a novel deep learning framework called TraffNet that learns the\\ncausality of traffic volume from vehicle trajectory data. First, we use a\\nheterogeneous graph to represent the road network, allowing the model to\\nincorporate causal features of traffic volumes. Next, motivated by the traffic\\ndomain knowledge, we propose a traffic causality learning method to learn an\\nembedding vector that encodes travel demands and path-level dependencies for\\neach road segment. Then, we model temporal dependencies to match the underlying\\nprocess of traffic generation. Finally, the experiments verify the utility of\\nTraffNet. The code of TraffNet is available at\\nhttps://github.com/mayunyi-1999/TraffNet_code.git.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15954v1', 'http://arxiv.org/pdf/2303.15954v1'], 'http://arxiv.org/pdf/2303.15954v1']\n",
      "['http://arxiv.org/abs/2303.15953v1', 'Randomly Initialized Subnetworks with Iterative Weight Recycling', ['Matt Gorbett', 'Darrell Whitley'], datetime.datetime(2023, 3, 28, 13, 12, tzinfo=datetime.timezone.utc), 'The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized\\nneural networks contain several subnetworks that achieve comparable accuracy to\\nfully trained models of the same architecture. However, current methods require\\nthat the network is sufficiently overparameterized. In this work, we propose a\\nmodification to two state-of-the-art algorithms (Edge-Popup and Biprop) that\\nfinds high-accuracy subnetworks with no additional storage cost or scaling. The\\nalgorithm, Iterative Weight Recycling, identifies subsets of important weights\\nwithin a randomly initialized network for intra-layer reuse. Empirically we\\nshow improvements on smaller network architectures and higher prune rates,\\nfinding that model sparsity can be increased through the \"recycling\" of\\nexisting weights. In addition to Iterative Weight Recycling, we complement the\\nMulti-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy,\\nrandomly initialized subnetwork\\'s produce diverse masks, despite being\\ngenerated with the same hyperparameter\\'s and pruning strategy. We explore the\\nlandscapes of these masks, which show high variability.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15953v1', 'http://arxiv.org/pdf/2303.15953v1'], 'http://arxiv.org/pdf/2303.15953v1']\n",
      "['http://arxiv.org/abs/2303.15947v1', 'Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings', ['Ryo Hachiuma', 'Tomohiro Shimizu', 'Hideo Saito', 'Hiroki Kajita', 'Yoshifumi Takatsume'], datetime.datetime(2023, 3, 28, 13, 0, 8, tzinfo=datetime.timezone.utc), \"Recording surgery in operating rooms is an essential task for education and\\nevaluation of medical treatment. However, recording the desired targets, such\\nas the surgery field, surgical tools, or doctor's hands, is difficult because\\nthe targets are heavily occluded during surgery. We use a recording system in\\nwhich multiple cameras are embedded in the surgical lamp, and we assume that at\\nleast one camera is recording the target without occlusion at any given time.\\nAs the embedded cameras obtain multiple video sequences, we address the task of\\nselecting the camera with the best view of the surgery. Unlike the conventional\\nmethod, which selects the camera based on the area size of the surgery field,\\nwe propose a deep neural network that predicts the camera selection probability\\nfrom multiple video sequences by learning the supervision of the expert\\nannotation. We created a dataset in which six different types of plastic\\nsurgery are recorded, and we provided the annotation of camera switching. Our\\nexperiments show that our approach successfully switched between cameras and\\noutperformed three baseline methods.\", 'cs.CV', ['cs.CV', 'cs.AI', 'cs.LG'], ['http://arxiv.org/abs/2303.15947v1', 'http://arxiv.org/pdf/2303.15947v1'], 'http://arxiv.org/pdf/2303.15947v1']\n",
      "['http://arxiv.org/abs/2303.15942v1', 'Globally Intelligent Adaptive Finite-/Fixed- Time Tracking Control for Strict-Feedback Nonlinear Systems via Composite Learning Approaches', ['Xidong Wang'], datetime.datetime(2023, 3, 28, 12, 55, 15, tzinfo=datetime.timezone.utc), \"This article focuses on the globally composite adaptive law-based intelligent\\nfinite-/fixed- time (FnT/FxT) tracking control issue for a family of uncertain\\nstrict-feedback nonlinear systems. First, intelligent approximators with new\\ncomposite updating laws are developed to model uncertain nonlinear terms, which\\nencompass prediction errors to enhance intelligent approximators' learning\\nbehaviors and fewer online learning parameters to diminish computational\\nburden. Then, a novel smooth switching function coupled with robust controllers\\nis designed to pull system states back when the transients are out of the\\napproximators' active domain. After that, a modified FnT/FxT backstepping\\ntechnique is constructed to render output to follow the reference trajectory,\\nand an adaptive law is employed to alleviate the impact of external\\ndisturbances. It is theoretically confirmed that the proposed control\\nstrategies ensure globally FnT/FxT boundedness of all the closed-loop\\nvariables. Finally, the validity of theoretical results is testified via a\\nsimulation case.\", 'eess.SY', ['eess.SY', 'cs.SY'], ['http://arxiv.org/abs/2303.15942v1', 'http://arxiv.org/pdf/2303.15942v1'], 'http://arxiv.org/pdf/2303.15942v1']\n",
      "['http://arxiv.org/abs/2303.15939v1', 'Physics-guided adversarial networks for artificial digital image correlation data generation', ['David Melching', 'Erik Schultheis', 'Eric Breitbarth'], datetime.datetime(2023, 3, 28, 12, 52, 40, tzinfo=datetime.timezone.utc), 'Digital image correlation (DIC) has become a valuable tool in the evaluation\\nof mechanical experiments, particularly fatigue crack growth experiments. The\\nevaluation requires accurate information of the crack path and crack tip\\nposition, which is difficult to obtain due to inherent noise and artefacts.\\nMachine learning models have been extremely successful in recognizing this\\nrelevant information given labelled DIC displacement data. For the training of\\nrobust models, which generalize well, big data is needed. However, data is\\ntypically scarce in the field of material science and engineering because\\nexperiments are expensive and time-consuming. We present a method to generate\\nsynthetic DIC displacement data using generative adversarial networks with a\\nphysics-guided discriminator. To decide whether data samples are real or fake,\\nthis discriminator additionally receives the derived von Mises equivalent\\nstrain. We show that this physics-guided approach leads to improved results in\\nterms of visual quality of samples, sliced Wasserstein distance, and geometry\\nscore.', 'eess.IV', ['eess.IV', 'cs.LG'], ['http://arxiv.org/abs/2303.15939v1', 'http://arxiv.org/pdf/2303.15939v1'], 'http://arxiv.org/pdf/2303.15939v1']\n",
      "['http://arxiv.org/abs/2303.15935v1', 'When Brain-inspired AI Meets AGI', ['Lin Zhao', 'Lu Zhang', 'Zihao Wu', 'Yuzhong Chen', 'Haixing Dai', 'Xiaowei Yu', 'Zhengliang Liu', 'Tuo Zhang', 'Xintao Hu', 'Xi Jiang', 'Xiang Li', 'Dajiang Zhu', 'Dinggang Shen', 'Tianming Liu'], datetime.datetime(2023, 3, 28, 12, 46, 38, tzinfo=datetime.timezone.utc), 'Artificial General Intelligence (AGI) has been a long-standing goal of\\nhumanity, with the aim of creating machines capable of performing any\\nintellectual task that humans can do. To achieve this, AGI researchers draw\\ninspiration from the human brain and seek to replicate its principles in\\nintelligent machines. Brain-inspired artificial intelligence is a field that\\nhas emerged from this endeavor, combining insights from neuroscience,\\npsychology, and computer science to develop more efficient and powerful AI\\nsystems. In this article, we provide a comprehensive overview of brain-inspired\\nAI from the perspective of AGI. We begin with the current progress in\\nbrain-inspired AI and its extensive connection with AGI. We then cover the\\nimportant characteristics for both human intelligence and AGI (e.g., scaling,\\nmultimodality, and reasoning). We discuss important technologies toward\\nachieving AGI in current AI systems, such as in-context learning and prompt\\ntuning. We also investigate the evolution of AGI systems from both algorithmic\\nand infrastructural perspectives. Finally, we explore the limitations and\\nfuture of AGI.', 'cs.AI', ['cs.AI'], ['http://arxiv.org/abs/2303.15935v1', 'http://arxiv.org/pdf/2303.15935v1'], 'http://arxiv.org/pdf/2303.15935v1']\n",
      "['http://arxiv.org/abs/2303.15916v1', 'From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification', ['Dominique Mercier', 'Andreas Dengel', 'Sheraz Ahmed'], datetime.datetime(2023, 3, 28, 12, 10, 45, tzinfo=datetime.timezone.utc), 'Deep learning has proven to be successful in various domains and for\\ndifferent tasks. However, when it comes to private data several restrictions\\nare making it difficult to use deep learning approaches in these application\\nfields. Recent approaches try to generate data privately instead of applying a\\nprivacy-preserving mechanism directly, on top of the classifier. The solution\\nis to create public data from private data in a manner that preserves the\\nprivacy of the data. In this work, two very prominent GAN-based architectures\\nwere evaluated in the context of private time series classification. In\\ncontrast to previous work, mostly limited to the image domain, the scope of\\nthis benchmark was the time series domain. The experiments show that especially\\nGSWGAN performs well across a variety of public datasets outperforming the\\ncompetitor DPWGAN. An analysis of the generated datasets further validates the\\nsuperiority of GSWGAN in the context of time series generation.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15916v1', 'http://arxiv.org/pdf/2303.15916v1'], 'http://arxiv.org/pdf/2303.15916v1']\n",
      "['http://arxiv.org/abs/2303.15904v1', 'Mask-Free Video Instance Segmentation', ['Lei Ke', 'Martin Danelljan', 'Henghui Ding', 'Yu-Wing Tai', 'Chi-Keung Tang', 'Fisher Yu'], datetime.datetime(2023, 3, 28, 11, 48, 7, tzinfo=datetime.timezone.utc), 'The recent advancement in Video Instance Segmentation (VIS) has largely been\\ndriven by the use of deeper and increasingly data-hungry transformer-based\\nmodels. However, video masks are tedious and expensive to annotate, limiting\\nthe scale and diversity of existing VIS datasets. In this work, we aim to\\nremove the mask-annotation requirement. We propose MaskFreeVIS, achieving\\nhighly competitive VIS performance, while only using bounding box annotations\\nfor the object state. We leverage the rich temporal mask consistency\\nconstraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss),\\nproviding strong mask supervision without any labels. Our TK-Loss finds\\none-to-many matches across frames, through an efficient patch-matching step\\nfollowed by a K-nearest neighbor selection. A consistency loss is then enforced\\non the found matches. Our mask-free objective is simple to implement, has no\\ntrainable parameters, is computationally efficient, yet outperforms baselines\\nemploying, e.g., state-of-the-art optical flow to enforce temporal mask\\nconsistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS and\\nBDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of our\\nmethod by drastically narrowing the gap between fully and weakly-supervised VIS\\nperformance. Our code and trained models are available at\\nhttps://github.com/SysCV/MaskFreeVis.', 'cs.CV', ['cs.CV', 'cs.AI'], ['http://arxiv.org/abs/2303.15904v1', 'http://arxiv.org/pdf/2303.15904v1'], 'http://arxiv.org/pdf/2303.15904v1']\n",
      "['http://arxiv.org/abs/2303.15901v1', 'Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm', ['Bakary Badjie', 'José Cecílio', 'António Casimiro'], datetime.datetime(2023, 3, 28, 11, 34, 54, tzinfo=datetime.timezone.utc), \"Adversarial attacks significantly threaten the robustness of deep neural\\nnetworks (DNNs). Despite the multiple defensive methods employed, they are\\nnevertheless vulnerable to poison attacks, where attackers meddle with the\\ninitial training data. In order to defend DNNs against such adversarial\\nattacks, this work proposes a novel method that combines the defensive\\ndistillation mechanism with a denoising autoencoder (DAE). This technique tries\\nto lower the sensitivity of the distilled model to poison attacks by spotting\\nand reconstructing poisonous adversarial inputs in the training data. We added\\ncarefully created adversarial samples to the initial training data to assess\\nthe proposed method's performance. Our experimental findings demonstrate that\\nour method successfully identified and reconstructed the poisonous inputs while\\nalso considering enhancing the DNN's resilience. The proposed approach provides\\na potent and robust defense mechanism for DNNs in various applications where\\ndata poisoning attacks are a concern. Thus, the defensive distillation\\ntechnique's limitation posed by poisonous adversarial attacks is overcome.\", 'cs.LG', ['cs.LG', 'cs.AI', 'cs.CR', '68', 'F.4.1'], ['http://arxiv.org/abs/2303.15901v1', 'http://arxiv.org/pdf/2303.15901v1'], 'http://arxiv.org/pdf/2303.15901v1']\n",
      "['http://arxiv.org/abs/2303.15888v1', 'Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning', ['Antonio Carta', 'Andrea Cossu', 'Vincenzo Lomonaco', 'Davide Bacciu', 'Joost van de Weijer'], datetime.datetime(2023, 3, 28, 11, 4, 17, tzinfo=datetime.timezone.utc), \"Distributed learning on the edge often comprises self-centered devices (SCD)\\nwhich learn local tasks independently and are unwilling to contribute to the\\nperformance of other SDCs. How do we achieve forward transfer at zero cost for\\nthe single SCDs? We formalize this problem as a Distributed Continual Learning\\nscenario, where SCD adapt to local tasks and a CL model consolidates the\\nknowledge from the resulting stream of models without looking at the SCD's\\nprivate data. Unfortunately, current CL methods are not directly applicable to\\nthis scenario. We propose Data-Agnostic Consolidation (DAC), a novel double\\nknowledge distillation method that consolidates the stream of SC models without\\nusing the original data. DAC performs distillation in the latent space via a\\nnovel Projected Latent Distillation loss. Experimental results show that DAC\\nenables forward transfer between SCDs and reaches state-of-the-art accuracy on\\nSplit CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and\\ndistributed CL scenarios. Somewhat surprisingly, even a single\\nout-of-distribution image is sufficient as the only source of data during\\nconsolidation.\", 'cs.LG', ['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE'], ['http://arxiv.org/abs/2303.15888v1', 'http://arxiv.org/pdf/2303.15888v1'], 'http://arxiv.org/pdf/2303.15888v1']\n",
      "['http://arxiv.org/abs/2303.15871v1', 'Control Barrier Functions in Dynamic UAVs for Kinematic Obstacle Avoidance: A Collision Cone Approach', ['Manan Tayal', 'Shishir Kolathaya'], datetime.datetime(2023, 3, 28, 10, 26, 30, tzinfo=datetime.timezone.utc), \"Unmanned aerial vehicles (UAVs), specifically quadrotors, have revolutionized\\nvarious industries with their maneuverability and versatility, but their safe\\noperation in dynamic environments heavily relies on effective collision\\navoidance techniques. This paper introduces a novel technique for safely\\nnavigating a quadrotor along a desired route while avoiding kinematic\\nobstacles. The proposed approach employs control barrier functions and utilizes\\ncollision cones to ensure that the quadrotor's velocity and the obstacle's\\nvelocity always point away from each other. In particular, we propose a new\\nconstraint formulation that ensures that the relative velocity between the\\nquadrotor and the obstacle always avoids a cone of vectors that may lead to a\\ncollision. By showing that the proposed constraint is a valid control barrier\\nfunction (CBFs) for quadrotors, we are able to leverage on its real-time\\nimplementation via Quadratic Programs (QPs), called the CBF-QPs. We validate\\nthe effectiveness of the proposed CBF-QPs by demonstrating collision avoidance\\nwith moving obstacles under multiple scenarios. This is shown in the pybullet\\nsimulator.Furthermore we compare the proposed approach with CBF-QPs shown in\\nliterature, especially the well-known higher order CBF-QPs (HO-CBF-QPs), where\\nin we show that it is more conservative compared to the proposed approach. This\\ncomparison also shown in simulation in detail.\", 'cs.RO', ['cs.RO'], ['http://arxiv.org/abs/2303.15871v1', 'http://arxiv.org/pdf/2303.15871v1'], 'http://arxiv.org/pdf/2303.15871v1']\n",
      "['http://arxiv.org/abs/2303.15852v1', 'Exploring Deep Learning Methods for Classification of SAR Images: Towards NextGen Convolutions via Transformers', ['Aakash Singh', 'Vivek Kumar Singh'], datetime.datetime(2023, 3, 28, 9, 43, 58, tzinfo=datetime.timezone.utc), 'Images generated by high-resolution SAR have vast areas of application as\\nthey can work better in adverse light and weather conditions. One such area of\\napplication is in the military systems. This study is an attempt to explore the\\nsuitability of current state-of-the-art models introduced in the domain of\\ncomputer vision for SAR target classification (MSTAR). Since the application of\\nany solution produced for military systems would be strategic and real-time,\\naccuracy is often not the only criterion to measure its performance. Other\\nimportant parameters like prediction time and input resiliency are equally\\nimportant. The paper deals with these issues in the context of SAR images.\\nExperimental results show that deep learning models can be suitably applied in\\nthe domain of SAR image classification with the desired performance levels.', 'eess.IV', ['eess.IV', 'cs.CV', 'cs.LG'], ['http://dx.doi.org/10.1007/978-3-031-28183-9_18', 'http://arxiv.org/abs/2303.15852v1', 'http://arxiv.org/pdf/2303.15852v1'], 'http://arxiv.org/pdf/2303.15852v1']\n",
      "['http://arxiv.org/abs/2303.15848v1', '4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images', ['Zhuoran Zheng', 'Xiuyi Jia'], datetime.datetime(2023, 3, 28, 9, 39, 29, tzinfo=datetime.timezone.utc), 'Currently, mobile and IoT devices are in dire need of a series of methods to\\nenhance 4K images with limited resource expenditure. The absence of large-scale\\n4K benchmark datasets hampers progress in this area, especially for dehazing.\\nThe challenges in building ultra-high-definition (UHD) dehazing datasets are\\nthe absence of estimation methods for UHD depth maps, high-quality 4K depth\\nestimation datasets, and migration strategies for UHD haze images from\\nsynthetic to real domains. To address these problems, we develop a novel\\nsynthetic method to simulate 4K hazy images (including nighttime and daytime\\nscenes) from clear images, which first estimates the scene depth, simulates the\\nlight rays and object reflectance, then migrates the synthetic images to real\\ndomains by using a GAN, and finally yields the hazy effects on 4K resolution\\nimages. We wrap these synthesized images into a benchmark called the 4K-HAZE\\ndataset. Specifically, we design the CS-Mixer (an MLP-based model that\\nintegrates \\\\textbf{C}hannel domain and \\\\textbf{S}patial domain) to estimate the\\ndepth map of 4K clear images, the GU-Net to migrate a 4K synthetic image to the\\nreal hazy domain. The most appealing aspect of our approach (depth estimation\\nand domain migration) is the capability to run a 4K image on a single GPU with\\n24G RAM in real-time (33fps). Additionally, this work presents an objective\\nassessment of several state-of-the-art single-image dehazing methods that are\\nevaluated using the 4K-HAZE dataset. At the end of the paper, we discuss the\\nlimitations of the 4K-HAZE dataset and its social implications.', 'cs.CV', ['cs.CV', 'cs.AI'], ['http://arxiv.org/abs/2303.15848v1', 'http://arxiv.org/pdf/2303.15848v1'], 'http://arxiv.org/pdf/2303.15848v1']\n",
      "['http://arxiv.org/abs/2303.15846v1', 'Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes', ['Auke Elfrink', 'Iacopo Vagliano', 'Ameen Abu-Hanna', 'Iacer Calixto'], datetime.datetime(2023, 3, 28, 9, 36, 53, tzinfo=datetime.timezone.utc), 'We investigate different natural language processing (NLP) approaches based\\non contextualised word representations for the problem of early prediction of\\nlung cancer using free-text patient medical notes of Dutch primary care\\nphysicians. Because lung cancer has a low prevalence in primary care, we also\\naddress the problem of classification under highly imbalanced classes.\\nSpecifically, we use large Transformer-based pretrained language models (PLMs)\\nand investigate: 1) how \\\\textit{soft prompt-tuning} -- an NLP technique used to\\nadapt PLMs using small amounts of training data -- compares to standard model\\nfine-tuning; 2) whether simpler static word embedding models (WEMs) can be more\\nrobust compared to PLMs in highly imbalanced settings; and 3) how models fare\\nwhen trained on notes from a small number of patients. We find that 1)\\nsoft-prompt tuning is an efficient alternative to standard model fine-tuning;\\n2) PLMs show better discrimination but worse calibration compared to simpler\\nstatic word embedding models as the classification problem becomes more\\nimbalanced; and 3) results when training models on small number of patients are\\nmixed and show no clear differences between PLMs and WEMs. All our code is\\navailable open source in\\n\\\\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}.', 'cs.CL', ['cs.CL', 'cs.AI', 'cs.LG', 'I.2.7'], ['http://arxiv.org/abs/2303.15846v1', 'http://arxiv.org/pdf/2303.15846v1'], 'http://arxiv.org/pdf/2303.15846v1']\n",
      "['http://arxiv.org/abs/2303.15844v1', 'CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics', ['Olusanmi Hundogan', 'Xixi Lu', 'Yupei Du', 'Hajo A. Reijers'], datetime.datetime(2023, 3, 28, 9, 35, 25, tzinfo=datetime.timezone.utc), \"Predictive process analytics focuses on predicting future states, such as the\\noutcome of running process instances. These techniques often use machine\\nlearning models or deep learning models (such as LSTM) to make such\\npredictions. However, these deep models are complex and difficult for users to\\nunderstand. Counterfactuals answer ``what-if'' questions, which are used to\\nunderstand the reasoning behind the predictions. For example, what if instead\\nof emailing customers, customers are being called? Would this alternative lead\\nto a different outcome? Current methods to generate counterfactual sequences\\neither do not take the process behavior into account, leading to generating\\ninvalid or infeasible counterfactual process instances, or heavily rely on\\ndomain knowledge. In this work, we propose a general framework that uses\\nevolutionary methods to generate counterfactual sequences. Our framework does\\nnot require domain knowledge. Instead, we propose to train a Markov model to\\ncompute the feasibility of generated counterfactual sequences and adapt three\\nother measures (delta in outcome prediction, similarity, and sparsity) to\\nensure their overall viability. The evaluation shows that we generate viable\\ncounterfactual sequences, outperform baseline methods in viability, and yield\\nsimilar results when compared to the state-of-the-art method that requires\\ndomain knowledge.\", 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15844v1', 'http://arxiv.org/pdf/2303.15844v1'], 'http://arxiv.org/pdf/2303.15844v1']\n",
      "['http://arxiv.org/abs/2303.15834v1', 'Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning', ['Robin Hirt', 'Niklas Kühl', 'Dominik Martin', 'Gerhard Satzger'], datetime.datetime(2023, 3, 28, 9, 6, 28, tzinfo=datetime.timezone.utc), 'Successful analytics solutions that provide valuable insights often hinge on\\nthe connection of various data sources. While it is often feasible to generate\\nlarger data pools within organizations, the application of analytics within\\n(inter-organizational) business networks is still severely constrained. As data\\nis distributed across several legal units, potentially even across countries,\\nthe fear of disclosing sensitive information as well as the sheer volume of the\\ndata that would need to be exchanged are key inhibitors for the creation of\\neffective system-wide solutions -- all while still reaching superior prediction\\nperformance. In this work, we propose a meta machine learning method that deals\\nwith these obstacles to enable comprehensive analyses within a business\\nnetwork. We follow a design science research approach and evaluate our method\\nwith respect to feasibility and performance in an industrial use case. First,\\nwe show that it is feasible to perform network-wide analyses that preserve data\\nconfidentiality as well as limit data transfer volume. Second, we demonstrate\\nthat our method outperforms a conventional isolated analysis and even gets\\nclose to a (hypothetical) scenario where all data could be shared within the\\nnetwork. Thus, we provide a fundamental contribution for making business\\nnetworks more effective, as we remove a key obstacle to tap the huge potential\\nof learning from data that is scattered throughout the network.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15834v1', 'http://arxiv.org/pdf/2303.15834v1'], 'http://arxiv.org/pdf/2303.15834v1']\n",
      "['http://arxiv.org/abs/2303.15833v1', 'Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning', ['Wonguk Cho', 'Jinha Park', 'Taesup Kim'], datetime.datetime(2023, 3, 28, 9, 5, 15, tzinfo=datetime.timezone.utc), 'Continual domain shift poses a significant challenge in real-world\\napplications, particularly in situations where labeled data is not available\\nfor new domains. The challenge of acquiring knowledge in this problem setting\\nis referred to as unsupervised continual domain shift learning. Existing\\nmethods for domain adaptation and generalization have limitations in addressing\\nthis issue, as they focus either on adapting to a specific domain or\\ngeneralizing to unseen domains, but not both. In this paper, we propose\\nComplementary Domain Adaptation and Generalization (CoDAG), a simple yet\\neffective learning framework that combines domain adaptation and generalization\\nin a complementary manner to achieve three major goals of unsupervised\\ncontinual domain shift learning: adapting to a current domain, generalizing to\\nunseen domains, and preventing forgetting of previously seen domains. Our\\napproach is model-agnostic, meaning that it is compatible with any existing\\ndomain adaptation and generalization algorithms. We evaluate CoDAG on several\\nbenchmark datasets and demonstrate that our model outperforms state-of-the-art\\nmodels in all datasets and evaluation metrics, highlighting its effectiveness\\nand robustness in handling unsupervised continual domain shift learning.', 'cs.LG', ['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML'], ['http://arxiv.org/abs/2303.15833v1', 'http://arxiv.org/pdf/2303.15833v1'], 'http://arxiv.org/pdf/2303.15833v1']\n",
      "['http://arxiv.org/abs/2303.15832v1', 'The transformative potential of machine learning for experiments in fluid mechanics', ['Ricardo Vinuesa', 'Steven L. Brunton', 'Beverley J. McKeon'], datetime.datetime(2023, 3, 28, 9, 3, 47, tzinfo=datetime.timezone.utc), 'The field of machine learning has rapidly advanced the state of the art in\\nmany fields of science and engineering, including experimental fluid dynamics,\\nwhich is one of the original big-data disciplines. This perspective will\\nhighlight several aspects of experimental fluid mechanics that stand to benefit\\nfrom progress advances in machine learning, including: 1) augmenting the\\nfidelity and quality of measurement techniques, 2) improving experimental\\ndesign and surrogate digital-twin models and 3) enabling real-time estimation\\nand control. In each case, we discuss recent success stories and ongoing\\nchallenges, along with caveats and limitations, and outline the potential for\\nnew avenues of ML-augmented and ML-enabled experimental fluid mechanics.', 'physics.flu-dyn', ['physics.flu-dyn', 'cs.AI'], ['http://arxiv.org/abs/2303.15832v1', 'http://arxiv.org/pdf/2303.15832v1'], 'http://arxiv.org/pdf/2303.15832v1']\n",
      "['http://arxiv.org/abs/2303.15826v1', 'MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation', ['Ziyuan Zhao', 'Kaixin Xu', 'Huai Zhe Yeo', 'Xulei Yang', 'Cuntai Guan'], datetime.datetime(2023, 3, 28, 8, 55, tzinfo=datetime.timezone.utc), 'Domain shift has been a long-standing issue for medical image segmentation.\\nRecently, unsupervised domain adaptation (UDA) methods have achieved promising\\ncross-modality segmentation performance by distilling knowledge from a\\nlabel-rich source domain to a target domain without labels. In this work, we\\npropose a multi-scale self-ensembling based UDA framework for automatic\\nsegmentation of two key brain structures i.e., Vestibular Schwannoma (VS) and\\nCochlea on high-resolution T2 images. First, a segmentation-enhanced\\ncontrastive unpaired image translation module is designed for image-level\\ndomain adaptation from source T1 to target T2. Next, multi-scale deep\\nsupervision and consistency regularization are introduced to a mean teacher\\nnetwork for self-ensemble learning to further close the domain gap.\\nFurthermore, self-training and intensity augmentation techniques are utilized\\nto mitigate label scarcity and boost cross-modality segmentation performance.\\nOur method demonstrates promising segmentation performance with a mean Dice\\nscore of 83.8% and 81.4% and an average asymmetric surface distance (ASSD) of\\n0.55 mm and 0.26 mm for the VS and Cochlea, respectively in the validation\\nphase of the crossMoDA 2022 challenge.', 'eess.IV', ['eess.IV', 'cs.AI', 'cs.CV'], ['http://arxiv.org/abs/2303.15826v1', 'http://arxiv.org/pdf/2303.15826v1'], 'http://arxiv.org/pdf/2303.15826v1']\n",
      "['http://arxiv.org/abs/2303.15823v1', 'Automated wildlife image classification: An active learning tool for ecological applications', ['Ludwig Bothmann', 'Lisa Wimmer', 'Omid Charrakh', 'Tobias Weber', 'Hendrik Edelhoff', 'Wibke Peters', 'Hien Nguyen', 'Caryl Benjamin', 'Annette Menzel'], datetime.datetime(2023, 3, 28, 8, 51, 15, tzinfo=datetime.timezone.utc), 'Wildlife camera trap images are being used extensively to investigate animal\\nabundance, habitat associations, and behavior, which is complicated by the fact\\nthat experts must first classify the images manually. Artificial intelligence\\nsystems can take over this task but usually need a large number of\\nalready-labeled training images to achieve sufficient performance. This\\nrequirement necessitates human expert labor and poses a particular challenge\\nfor projects with few cameras or short durations. We propose a label-efficient\\nlearning strategy that enables researchers with small or medium-sized image\\ndatabases to leverage the potential of modern machine learning, thus freeing\\ncrucial resources for subsequent analyses.\\n  Our methodological proposal is two-fold: (1) We improve current strategies of\\ncombining object detection and image classification by tuning the\\nhyperparameters of both models. (2) We provide an active learning (AL) system\\nthat allows training deep learning models very efficiently in terms of required\\nhuman-labeled training images. We supply a software package that enables\\nresearchers to use these methods directly and thereby ensure the broad\\napplicability of the proposed framework in ecological practice.\\n  We show that our tuning strategy improves predictive performance. We\\ndemonstrate how the AL pipeline reduces the amount of pre-labeled data needed\\nto achieve a specific predictive performance and that it is especially valuable\\nfor improving out-of-sample predictive performance.\\n  We conclude that the combination of tuning and AL increases predictive\\nperformance substantially. Furthermore, we argue that our work can broadly\\nimpact the community through the ready-to-use software package provided.\\nFinally, the publication of our models tailored to European wildlife data\\nenriches existing model bases mostly trained on data from Africa and North\\nAmerica.', 'cs.CV', ['cs.CV', 'stat.AP'], ['http://arxiv.org/abs/2303.15823v1', 'http://arxiv.org/pdf/2303.15823v1'], 'http://arxiv.org/pdf/2303.15823v1']\n",
      "['http://arxiv.org/abs/2303.15822v1', 'One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization', ['Deze Wang', 'Boxing Chen', 'Shanshan Li', 'Wei Luo', 'Shaoliang Peng', 'Wei Dong', 'Xiangke Liao'], datetime.datetime(2023, 3, 28, 8, 49, 54, tzinfo=datetime.timezone.utc), 'As pre-trained models automate many code intelligence tasks, a widely used\\nparadigm is to fine-tune a model on the task dataset for each programming\\nlanguage. A recent study reported that multilingual fine-tuning benefits a\\nrange of tasks and models. However, we find that multilingual fine-tuning leads\\nto performance degradation on recent models UniXcoder and CodeT5.\\n  To alleviate the potentially catastrophic forgetting issue in multilingual\\nmodels, we fix all pre-trained model parameters, insert the parameter-efficient\\nstructure adapter, and fine-tune it. Updating only 0.6\\\\% of the overall\\nparameters compared to full-model fine-tuning for each programming language,\\nadapter tuning yields consistent improvements on code search and summarization\\ntasks, achieving state-of-the-art results. In addition, we experimentally show\\nits effectiveness in cross-lingual and low-resource scenarios. Multilingual\\nfine-tuning with 200 samples per programming language approaches the results\\nfine-tuned with the entire dataset on code summarization. Our experiments on\\nthree probing tasks show that adapter tuning significantly outperforms\\nfull-model fine-tuning and effectively overcomes catastrophic forgetting.', 'cs.SE', ['cs.SE', 'cs.AI'], ['http://arxiv.org/abs/2303.15822v1', 'http://arxiv.org/pdf/2303.15822v1'], 'http://arxiv.org/pdf/2303.15822v1']\n",
      "['http://arxiv.org/abs/2303.15818v1', 'Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition', ['Xiao Yang', 'Chang Liu', 'Longlong Xu', 'Yikai Wang', 'Yinpeng Dong', 'Ning Chen', 'Hang Su', 'Jun Zhu'], datetime.datetime(2023, 3, 28, 8, 42, 54, tzinfo=datetime.timezone.utc), \"Face recognition is a prevailing authentication solution in numerous\\nbiometric applications. Physical adversarial attacks, as an important\\nsurrogate, can identify the weaknesses of face recognition systems and evaluate\\ntheir robustness before deployed. However, most existing physical attacks are\\neither detectable readily or ineffective against commercial recognition\\nsystems. The goal of this work is to develop a more reliable technique that can\\ncarry out an end-to-end evaluation of adversarial robustness for commercial\\nsystems. It requires that this technique can simultaneously deceive black-box\\nrecognition models and evade defensive mechanisms. To fulfill this, we design\\nadversarial textured 3D meshes (AT3D) with an elaborate topology on a human\\nface, which can be 3D-printed and pasted on the attacker's face to evade the\\ndefenses. However, the mesh-based optimization regime calculates gradients in\\nhigh-dimensional mesh space, and can be trapped into local optima with\\nunsatisfactory transferability. To deviate from the mesh-based space, we\\npropose to perturb the low-dimensional coefficient space based on 3D Morphable\\nModel, which significantly improves black-box transferability meanwhile\\nenjoying faster search efficiency and better visual quality. Extensive\\nexperiments in digital and physical scenarios show that our method effectively\\nexplores the security vulnerabilities of multiple popular commercial services,\\nincluding three recognition APIs, four anti-spoofing APIs, two prevailing\\nmobile phones and two automated access control systems.\", 'cs.CV', ['cs.CV', 'cs.AI'], ['http://arxiv.org/abs/2303.15818v1', 'http://arxiv.org/pdf/2303.15818v1'], 'http://arxiv.org/pdf/2303.15818v1']\n",
      "['http://arxiv.org/abs/2303.15810v1', 'Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization', ['Haoran Xu', 'Li Jiang', 'Jianxiong Li', 'Zhuoran Yang', 'Zhaoran Wang', 'Victor Wai Kin Chan', 'Xianyuan Zhan'], datetime.datetime(2023, 3, 28, 8, 30, 1, tzinfo=datetime.timezone.utc), 'Most offline reinforcement learning (RL) methods suffer from the trade-off\\nbetween improving the policy to surpass the behavior policy and constraining\\nthe policy to limit the deviation from the behavior policy as computing\\n$Q$-values using out-of-distribution (OOD) actions will suffer from errors due\\nto distributional shift. The recently proposed \\\\textit{In-sample Learning}\\nparadigm (i.e., IQL), which improves the policy by quantile regression using\\nonly data samples, shows great promise because it learns an optimal policy\\nwithout querying the value function of any unseen actions. However, it remains\\nunclear how this type of method handles the distributional shift in learning\\nthe value function. In this work, we make a key finding that the in-sample\\nlearning paradigm arises under the \\\\textit{Implicit Value Regularization} (IVR)\\nframework. This gives a deeper understanding of why the in-sample learning\\nparadigm works, i.e., it applies implicit value regularization to the policy.\\nBased on the IVR framework, we further propose two practical algorithms, Sparse\\n$Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the same\\nvalue regularization used in existing works, but in a complete in-sample\\nmanner. Compared with IQL, we find that our algorithms introduce sparsity in\\nlearning the value function, making them more robust in noisy data regimes. We\\nalso verify the effectiveness of SQL and EQL on D4RL benchmark datasets and\\nshow the benefits of in-sample learning by comparing them with CQL in small\\ndata regimes.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15810v1', 'http://arxiv.org/pdf/2303.15810v1'], 'http://arxiv.org/pdf/2303.15810v1']\n",
      "['http://arxiv.org/abs/2303.15794v1', 'Machine Learning for Observational Cosmology', ['Kana Moriwaki', 'Takahiro Nishimichi', 'Naoki Yoshida'], datetime.datetime(2023, 3, 28, 7, 59, 4, tzinfo=datetime.timezone.utc), 'An array of large observational programs using ground-based and space-borne\\ntelescopes is planned in the next decade. The forthcoming wide-field sky\\nsurveys are expected to deliver a sheer volume of data exceeding an exabyte.\\nProcessing the large amount of multiplex astronomical data is technically\\nchallenging, and fully automated technologies based on machine learning and\\nartificial intelligence are urgently needed. Maximizing scientific returns from\\nthe big data requires community-wide efforts. We summarize recent progress in\\nmachine learning applications in observational cosmology. We also address\\ncrucial issues in high-performance computing that are needed for the data\\nprocessing and statistical analysis.', 'astro-ph.IM', ['astro-ph.IM', 'astro-ph.CO', 'astro-ph.GA', 'astro-ph.HE'], ['http://arxiv.org/abs/2303.15794v1', 'http://arxiv.org/pdf/2303.15794v1'], 'http://arxiv.org/pdf/2303.15794v1']\n",
      "['http://arxiv.org/abs/2303.15772v1', 'Ecosystem Graphs: The Social Footprint of Foundation Models', ['Rishi Bommasani', 'Dilara Soylu', 'Thomas I. Liao', 'Kathleen A. Creel', 'Percy Liang'], datetime.datetime(2023, 3, 28, 7, 18, 29, tzinfo=datetime.timezone.utc), 'Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence\\nsociety, warranting immediate social attention. While the models themselves\\ngarner much attention, to accurately characterize their impact, we must\\nconsider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a\\ndocumentation framework to transparently centralize knowledge of this\\necosystem. Ecosystem Graphs is composed of assets (datasets, models,\\napplications) linked together by dependencies that indicate technical (e.g. how\\nBing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI)\\nrelationships. To supplement the graph structure, each asset is further\\nenriched with fine-grained metadata (e.g. the license or training emissions).\\nWe document the ecosystem extensively at\\nhttps://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate\\n262 assets (64 datasets, 128 models, 70 applications) from 63 organizations\\nlinked by 356 dependencies. We show Ecosystem Graphs functions as a powerful\\nabstraction and interface for achieving the minimum transparency required to\\naddress myriad use cases. Therefore, we envision Ecosystem Graphs will be a\\ncommunity-maintained resource that provides value to stakeholders spanning AI\\nresearchers, industry professionals, social scientists, auditors and\\npolicymakers.', 'cs.LG', ['cs.LG', 'cs.AI', 'cs.CY'], ['http://arxiv.org/abs/2303.15772v1', 'http://arxiv.org/pdf/2303.15772v1'], 'http://arxiv.org/pdf/2303.15772v1']\n",
      "['http://arxiv.org/abs/2303.15768v1', 'RobustSwap: A Simple yet Robust Face Swapping Model against Attribute Leakage', ['Jaeseong Lee', 'Taewoo Kim', 'Sunghyun Park', 'Younggun Lee', 'Jaegul Choo'], datetime.datetime(2023, 3, 28, 7, 3, 31, tzinfo=datetime.timezone.utc), \"Face swapping aims at injecting a source image's identity (i.e., facial\\nfeatures) into a target image, while strictly preserving the target's\\nattributes, which are irrelevant to identity. However, we observed that\\nprevious approaches still suffer from source attribute leakage, where the\\nsource image's attributes interfere with the target image's. In this paper, we\\nanalyze the latent space of StyleGAN and find the adequate combination of the\\nlatents geared for face swapping task. Based on the findings, we develop a\\nsimple yet robust face swapping model, RobustSwap, which is resistant to the\\npotential source attribute leakage. Moreover, we exploit the coordination of\\n3DMM's implicit and explicit information as a guidance to incorporate the\\nstructure of the source image and the precise pose of the target image. Despite\\nour method solely utilizing an image dataset without identity labels for\\ntraining, our model has the capability to generate high-fidelity and temporally\\nconsistent videos. Through extensive qualitative and quantitative evaluations,\\nwe demonstrate that our method shows significant improvements compared with the\\nprevious face swapping models in synthesizing both images and videos. Project\\npage is available at https://robustswap.github.io/\", 'cs.CV', ['cs.CV', 'cs.AI'], ['http://arxiv.org/abs/2303.15768v1', 'http://arxiv.org/pdf/2303.15768v1'], 'http://arxiv.org/pdf/2303.15768v1']\n",
      "['http://arxiv.org/abs/2303.15747v1', 'TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns', ['Soma Onishi', 'Kenta Oono', 'Kohei Hayashi'], datetime.datetime(2023, 3, 28, 6, 3, 41, tzinfo=datetime.timezone.utc), 'We present \\\\emph{TabRet}, a pre-trainable Transformer-based model for tabular\\ndata. TabRet is designed to work on a downstream task that contains columns not\\nseen in pre-training. Unlike other methods, TabRet has an extra learning step\\nbefore fine-tuning called \\\\emph{retokenizing}, which calibrates feature\\nembeddings based on the masked autoencoding loss. In experiments, we\\npre-trained TabRet with a large collection of public health surveys and\\nfine-tuned it on classification tasks in healthcare, and TabRet achieved the\\nbest AUC performance on four datasets. In addition, an ablation study shows\\nretokenizing and random shuffle augmentation of columns during pre-training\\ncontributed to performance gains.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15747v1', 'http://arxiv.org/pdf/2303.15747v1'], 'http://arxiv.org/pdf/2303.15747v1']\n",
      "['http://arxiv.org/abs/2303.15746v1', 'qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization', ['Raul Astudillo', 'Zhiyuan Jerry Lin', 'Eytan Bakshy', 'Peter I. Frazier'], datetime.datetime(2023, 3, 28, 6, 2, 56, tzinfo=datetime.timezone.utc), \"Preferential Bayesian optimization (PBO) is a framework for optimizing a\\ndecision maker's latent utility function using preference feedback. This work\\nintroduces the expected utility of the best option (qEUBO) as a novel\\nacquisition function for PBO. When the decision maker's responses are\\nnoise-free, we show that qEUBO is one-step Bayes optimal and thus equivalent to\\nthe popular knowledge gradient acquisition function. We also show that qEUBO\\nenjoys an additive constant approximation guarantee to the one-step\\nBayes-optimal policy when the decision maker's responses are corrupted by\\nnoise. We provide an extensive evaluation of qEUBO and demonstrate that it\\noutperforms the state-of-the-art acquisition functions for PBO across many\\nsettings. Finally, we show that, under sufficient regularity conditions,\\nqEUBO's Bayesian simple regret converges to zero at a rate $o(1/n)$ as the\\nnumber of queries, $n$, goes to infinity. In contrast, we show that simple\\nregret under qEI, a popular acquisition function for standard BO often used for\\nPBO, can fail to converge to zero. Enjoying superior performance, simple\\ncomputation, and a grounded decision-theoretic justification, qEUBO is a\\npromising acquisition function for PBO.\", 'cs.LG', ['cs.LG', 'stat.ML'], ['http://arxiv.org/abs/2303.15746v1', 'http://arxiv.org/pdf/2303.15746v1'], 'http://arxiv.org/pdf/2303.15746v1']\n",
      "['http://arxiv.org/abs/2303.15745v1', 'On Feature Scaling of Recursive Feature Machines', ['Arunav Gupta', 'Rohit Mishra', 'William Luu', 'Mehdi Bouassami'], datetime.datetime(2023, 3, 28, 6, 0, 41, tzinfo=datetime.timezone.utc), 'In this technical report, we explore the behavior of Recursive Feature\\nMachines (RFMs), a type of novel kernel machine that recursively learns\\nfeatures via the average gradient outer product, through a series of\\nexperiments on regression datasets. When successively adding random noise\\nfeatures to a dataset, we observe intriguing patterns in the Mean Squared Error\\n(MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern.\\nThis behavior is consistent across different dataset sizes, noise parameters,\\nand target functions. Interestingly, the observed MSE curves show similarities\\nto the \"double descent\" phenomenon observed in deep neural networks, hinting at\\nnew connection between RFMs and neural network behavior. This report lays the\\ngroundwork for future research into this peculiar behavior.', 'cs.LG', ['cs.LG', 'cs.AI'], ['http://arxiv.org/abs/2303.15745v1', 'http://arxiv.org/pdf/2303.15745v1'], 'http://arxiv.org/pdf/2303.15745v1']\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "for result in search.results():\n",
    "    article = []\n",
    "    article.append(result.entry_id)\n",
    "    article.append(result.title)\n",
    "    aa = []\n",
    "    for i in range(len(result.authors)):\n",
    "        aa.append(str(result.authors[i]))\n",
    "    article.append(aa)\n",
    "    article.append(result.published)\n",
    "    article.append(result.summary)\n",
    "    article.append(result.primary_category)\n",
    "    article.append(result.categories)\n",
    "    ll = []\n",
    "    for i in range(len(result.links)):\n",
    "        ll.append(str(result.links[i]))\n",
    "    article.append(ll)\n",
    "    article.append(result.pdf_url)\n",
    "    \n",
    "    articles.append(article)  \n",
    "    \n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a80ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e507ae37d243e3a689ab64359ec1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 10:56:32,594 - BERTopic - Transformed documents to Embeddings\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(verbose=True, embedding_model=\"paraphrase-MiniLM-L12-v2\", min_topic_size=10)\n",
    "topics, _ = topic_model.fit_transform(article); len(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be47d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
